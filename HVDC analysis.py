# run_hvdc_warehouse_analysis.py - HVDC ì°½ê³  ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸
"""
HVDC Warehouse Ontology ê¸°ë°˜ End-to-End Analysis Pipeline
ì‹¤í–‰: python run_hvdc_warehouse_analysis.py
ê²°ê³¼: HVDC_Comprehensive_Report.xlsx (15ê°œ ì‹œíŠ¸)

í•„ìš” íŒ¨í‚¤ì§€: pip install pandas openpyxl pydantic
"""

import glob, os, re, pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
from dataclasses import dataclass
import json
import warnings
warnings.filterwarnings('ignore')

# íƒ€ì„ë¼ì¸ ëª¨ë“ˆ import
try:
    import timeline_tracking_module as ttm
    TIMELINE_AVAILABLE = True
    print("âœ… Timeline ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    TIMELINE_AVAILABLE = False
    print("âš ï¸ Timeline ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„ì„ë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# =============================================================================
# 1. ONTOLOGY UTILS - HVDC Warehouse Ontology ê¸°ë°˜ ë§¤í•‘
# =============================================================================

# 1-1. ì°½ê³ (Warehouse/Location) ê·œì¹™ - í™•ì¥ëœ ë§¤í•‘
LOC_MAP = {
    r"M44.*": "DSV Indoor",      # ëƒ‰ë°©Â·ê³ ê°€í’ˆ
    r"M1.*": "DSV Al Markaz",    # í”¼í‚¹Â·ì¥ê¸°
    r"OUT.*": "DSV Outdoor",     # ì•¼ì  Cross-dock
    r"MOSB.*": "MOSB",           # Barge Load-out
    r"MZP.*": "DSV MZP",
    r".*Indoor.*": "DSV Indoor",
    r".*Outdoor.*": "DSV Outdoor",
    r".*Al.*Markaz.*": "DSV Al Markaz",
    r".*Markaz.*": "DSV Al Markaz",
    r"Hauler.*Indoor": "DSV Indoor",
    r"DHL.*WH": "DHL WH",
    r"AAA.*Storage": "AAA Storage",
    r"Shifting": "Shifting"
}

# 1-2. í˜„ì¥(Site/Project) ê·œì¹™
SITE_PATTERNS = {
    r".*AGI.*": "AGI",
    r".*DAS.*": "DAS", 
    r".*MIR.*": "MIR",
    r".*SHU.*": "SHU"
}

# 1-3. ì¸ë³´ì´ìŠ¤ Category â†’ Location ë§¤í•‘
CATEGORY_MAP = {
    "Indoor(M44)": "DSV Indoor",
    "Outdoor": "DSV Outdoor",
    "Al Markaz": "DSV Al Markaz",
    "DSV Indoor": "DSV Indoor",
    "DSV Outdoor": "DSV Outdoor", 
    "DSV Al Markaz": "DSV Al Markaz",
    "MOSB": "MOSB"
}

# 1-4. íŒŒì¼ íƒ€ì…ë³„ ì²˜ë¦¬ ê·œì¹™
FILE_TYPE_PATTERNS = {
    'BL': [r'.*\(HE\)\.xlsx$', r'.*HITACHI.*\.xlsx$', r'.*SIMENSE.*\.xlsx$'],
    'MIR': [r'.*LOCAL.*\.xlsx$'],
    'PICK': [r'.*(0214|0252).*\.xlsx$'],
    'INVOICE': [r'.*Invoice.*\.xlsx$'],
    'ONHAND': [r'.*Stock.*OnHand.*\.xlsx$', r'.*OnHand.*\.xlsx$']
}

def map_loc(raw_code: Union[str, None]) -> str:
    """
    Regex-based Location í‘œì¤€í™” (30ms ë‚´ ì²˜ë¦¬)
    >>> map_loc('M44-BAY07') â†’ 'DSV Indoor'
    """
    if pd.isna(raw_code):
        return "UNKNOWN"
    
    raw_str = str(raw_code).strip()
    for pattern, canonical in LOC_MAP.items():
        if re.fullmatch(pattern, raw_str, re.IGNORECASE):
            return canonical
    return raw_str

def map_site(loc_from: Union[str, None] = None, 
             loc_to: Union[str, None] = None, 
             site_col: Union[str, None] = None) -> str:
    """
    Site ì •ê·œí™” & Fallback
    ìš°ì„ ìˆœìœ„: Site ì»¬ëŸ¼ â†’ Loc_To â†’ Loc_From
    """
    candidates = [site_col, loc_to, loc_from]
    
    for candidate in candidates:
        if pd.notna(candidate):
            candidate_str = str(candidate).strip()
            for pattern, site in SITE_PATTERNS.items():
                if re.match(pattern, candidate_str, re.IGNORECASE):
                    return site
    return "UNK"

def map_category(cat: Union[str, None]) -> str:
    """
    Invoice Categoryë¥¼ Canonical Locationìœ¼ë¡œ ë³€í™˜
    """
    if pd.isna(cat):
        return "UNKNOWN"
    
    cat_str = str(cat).strip()
    return CATEGORY_MAP.get(cat_str, cat_str)

def detect_file_type(filepath: str) -> str:
    """íŒŒì¼ ê²½ë¡œë¡œ íŒŒì¼ íƒ€ì… ìë™ ê°ì§€"""
    filename = os.path.basename(filepath)
    
    for file_type, patterns in FILE_TYPE_PATTERNS.items():
        for pattern in patterns:
            if re.search(pattern, filename, re.IGNORECASE):
                return file_type
    
    return 'UNKNOWN'

def fuzzy_find_column(df: pd.DataFrame, patterns: List[str], threshold: float = 0.7) -> Optional[str]:
    """í¼ì§€ ë§¤ì¹­ìœ¼ë¡œ ì»¬ëŸ¼ ì°¾ê¸°"""
    df_cols_lower = [str(col).lower() for col in df.columns]
    
    # ì •í™•í•œ ë§¤ì¹­ ìš°ì„ 
    for pattern in patterns:
        for i, col_lower in enumerate(df_cols_lower):
            if pattern.lower() in col_lower:
                return df.columns[i]
    
    # ìœ ì‚¬ë„ ë§¤ì¹­
    from difflib import SequenceMatcher
    best_match = None
    best_ratio = 0
    
    for pattern in patterns:
        for i, col in enumerate(df.columns):
            ratio = SequenceMatcher(None, pattern.lower(), str(col).lower()).ratio()
            if ratio > best_ratio and ratio >= threshold:
                best_ratio = ratio
                best_match = df.columns[i]
    
    return best_match

# =============================================================================
# 2. INGESTOR V2 - íŒŒì¼ë³„ ë°ì´í„° ë¡œë”© ì—”ì§„
# =============================================================================

class DataExtractor:
    """í†µí•© ë°ì´í„° ì¶”ì¶œê¸°"""
    
    @staticmethod
    def extract_case_movements(df: pd.DataFrame, file_type: str, source_file: str) -> List[Dict]:
        """ì¼€ì´ìŠ¤ë³„ ì´ë™ ë°ì´í„° ì¶”ì¶œ"""
        movements = []
        
        # ê¸°ë³¸ ì»¬ëŸ¼ ë§¤í•‘
        case_col = fuzzy_find_column(df, ['case', 'carton', 'box', 'mr#', 'sct ship no.', 'case no'])
        qty_col = fuzzy_find_column(df, ["q'ty", 'qty', 'quantity', 'qty shipped', 'received'])
        
        if not case_col:
            print(f"   âš ï¸ Case ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_file}")
            return []
        
        # ì¹˜ìˆ˜ ì»¬ëŸ¼ ì°¾ê¸° ë° SQM ê³„ì‚°
        length_col = fuzzy_find_column(df, ['l(cm)', 'l(m)', 'length'])
        width_col = fuzzy_find_column(df, ['w(cm)', 'w(m)', 'width']) 
        height_col = fuzzy_find_column(df, ['h(cm)', 'h(m)', 'height'])
        
        # ì°½ê³  ë° ì‚¬ì´íŠ¸ ì»¬ëŸ¼ë“¤
        warehouse_cols = []
        site_cols = []
        
        for col in df.columns:
            col_str = str(col)
            if any(wh in col_str for wh in ['DSV', 'MOSB', 'Indoor', 'Outdoor', 'Markaz', 'MZP', 'Hauler', 'DHL', 'AAA', 'Shifting']):
                warehouse_cols.append(col)
            elif any(site in col_str for site in ['DAS', 'MIR', 'SHU', 'AGI']):
                site_cols.append(col)
        
        print(f"   ğŸ“¦ ë°œê²¬ëœ ì°½ê³  ì»¬ëŸ¼: {warehouse_cols}")
        print(f"   ğŸ—ï¸ ë°œê²¬ëœ ì‚¬ì´íŠ¸ ì»¬ëŸ¼: {site_cols}")
        
        # ê° í–‰ ì²˜ë¦¬
        for idx, row in df.iterrows():
            case_no = str(row[case_col]) if pd.notna(row[case_col]) else f"CASE_{idx}_{source_file}"
            qty = row[qty_col] if qty_col and pd.notna(row[qty_col]) else 1
            
            # SQM ê³„ì‚°
            sqm = 0
            if length_col and width_col:
                length_val = pd.to_numeric(row.get(length_col, 0), errors='coerce') or 0
                width_val = pd.to_numeric(row.get(width_col, 0), errors='coerce') or 0
                
                # cmë¥¼ më¡œ ë³€í™˜
                if length_col and '(cm)' in str(length_col).lower():
                    length_val = length_val / 100
                if width_col and '(cm)' in str(width_col).lower():
                    width_val = width_val / 100
                
                sqm = length_val * width_val * qty
            
            # CBM ê³„ì‚°
            cbm = 0
            if height_col and sqm > 0:
                height_val = pd.to_numeric(row.get(height_col, 0), errors='coerce') or 0
                if '(cm)' in str(height_col).lower():
                    height_val = height_val / 100
                cbm = sqm * height_val
            
            # ì°½ê³  ì´ë™ ê¸°ë¡
            for wh_col in warehouse_cols:
                if pd.notna(row[wh_col]):
                    date_val = pd.to_datetime(row[wh_col], errors='coerce')
                    if pd.notna(date_val):
                        movements.append({
                            'TxID': f"{case_no}_{wh_col}_{date_val.strftime('%Y%m%d')}",
                            'Case_No': case_no,
                            'Date': date_val,
                            'Loc_From': None,
                            'Loc_To': map_loc(wh_col),
                            'Site': map_site(site_col=wh_col),
                            'Qty': qty,
                            'SQM': sqm,
                            'CBM': cbm,
                            'Cost': 0,
                            'TxType': 'IN',
                            'SOURCE_FILE': source_file,
                            'FILE_TYPE': file_type
                        })
            
            # ì‚¬ì´íŠ¸ ë°°ì†¡ ê¸°ë¡
            for site_col in site_cols:
                if pd.notna(row[site_col]):
                    date_val = pd.to_datetime(row[site_col], errors='coerce')
                    if pd.notna(date_val):
                        movements.append({
                            'TxID': f"{case_no}_{site_col}_{date_val.strftime('%Y%m%d')}",
                            'Case_No': case_no,
                            'Date': date_val,
                            'Loc_From': 'WAREHOUSE',
                            'Loc_To': None,
                            'Site': map_site(site_col=site_col),
                            'Qty': qty,
                            'SQM': sqm,
                            'CBM': cbm,
                            'Cost': 0,
                            'TxType': 'OUT',
                            'SOURCE_FILE': source_file,
                            'FILE_TYPE': file_type
                        })
        
        return movements
    
    @staticmethod
    def load_warehouse_file(filepath: str) -> List[Dict]:
        """ì°½ê³  íŒŒì¼ ë¡œë”© (ê°œì„ ëœ ë²„ì „)"""
        try:
            print(f"   âœ… {os.path.basename(filepath)} ë¡œë”© ì‹œì‘...")
            
            # íŒŒì¼ íƒ€ì… ê°ì§€
            file_type = detect_file_type(filepath)
            print(f"   ğŸ“‹ íŒŒì¼ íƒ€ì…: {file_type}")
            
            # ì‹œíŠ¸ëª… ìë™ ê°ì§€
            try:
                # ë¨¼ì € ì²« ë²ˆì§¸ ì‹œíŠ¸ë¡œ ì‹œë„
                df = pd.read_excel(filepath, sheet_name=0, nrows=5)
                sheet_name = 0
            except Exception as e:
                print(f"   âš ï¸ ì²« ë²ˆì§¸ ì‹œíŠ¸ ì½ê¸° ì‹¤íŒ¨: {e}")
                # ì‹œíŠ¸ëª… ëª©ë¡ í™•ì¸
                try:
                    xl_file = pd.ExcelFile(filepath)
                    sheet_names = xl_file.sheet_names
                    print(f"   ğŸ“„ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œíŠ¸: {sheet_names}")
                    
                    # ë°ì´í„°ê°€ ìˆëŠ” ì‹œíŠ¸ ì°¾ê¸°
                    for sheet in sheet_names:
                        try:
                            test_df = pd.read_excel(filepath, sheet_name=sheet, nrows=5)
                            if not test_df.empty and len(test_df.columns) > 3:
                                sheet_name = sheet
                                print(f"   âœ… ì‹œíŠ¸ ì„ íƒ: {sheet}")
                                break
                        except:
                            continue
                    else:
                        sheet_name = 0  # ê¸°ë³¸ê°’
                except:
                    sheet_name = 0
            
            # ì „ì²´ ë°ì´í„° ë¡œë”©
            try:
                df = pd.read_excel(filepath, sheet_name=sheet_name)
                print(f"   âœ… {os.path.basename(filepath)} ë¡œë”© ì™„ë£Œ ({len(df)}í–‰, íƒ€ì…: {file_type})")
            except Exception as e:
                print(f"   âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                return []
            
            # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ì²´í¬
            if df.empty:
                print(f"   âš ï¸ ë¹ˆ ë°ì´í„°í”„ë ˆì„")
                return []
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬
            df.columns = [str(col).strip() for col in df.columns]
            
            # ë°ì´í„° ì¶”ì¶œ
            movements = DataExtractor.extract_case_movements(df, file_type, filepath)
            return movements
            
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    @staticmethod
    def load_invoice(filepath: str) -> List[Dict]:
        """ì¸ë³´ì´ìŠ¤ íŒŒì¼ ë¡œë”© (TxType='COST')"""
        try:
            xl_file = pd.ExcelFile(filepath)
            
            # ì¸ë³´ì´ìŠ¤ ì‹œíŠ¸ ì°¾ê¸°
            sheet_name = xl_file.sheet_names[0]
            for sheet in xl_file.sheet_names:
                if any(keyword in sheet.lower() for keyword in ['invoice', 'cost', 'billing']):
                    sheet_name = sheet
                    break
            
            df = pd.read_excel(filepath, sheet_name=sheet_name)
            
            # ì»¬ëŸ¼ ë§¤í•‘
            date_col = fuzzy_find_column(df, ['date', 'month', 'operation month', 'period'])
            category_col = fuzzy_find_column(df, ['category', 'location', 'warehouse', 'type'])
            cost_col = fuzzy_find_column(df, ['total', 'cost', 'amount', 'value', 'price'])
            case_col = fuzzy_find_column(df, ['case', 'case no', 'case number'])
            
            if not cost_col:
                print(f"   âš ï¸ Cost ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
                return []
            
            movements = []
            
            for idx, row in df.iterrows():
                # ë‚ ì§œ ì²˜ë¦¬
                if date_col and pd.notna(row[date_col]):
                    date_val = pd.to_datetime(row[date_col], errors='coerce')
                else:
                    date_val = datetime.now()
                
                # ìœ„ì¹˜ ë§¤í•‘
                location = "UNKNOWN"
                if category_col and pd.notna(row[category_col]):
                    location = map_category(row[category_col])
                
                # ë¹„ìš©
                cost_val = pd.to_numeric(row[cost_col], errors='coerce') or 0
                
                # ì¼€ì´ìŠ¤ ë²ˆí˜¸
                case_no = f"COST_{idx}_{os.path.basename(filepath)}"
                if case_col and pd.notna(row[case_col]):
                    case_no = str(row[case_col])
                
                movements.append({
                    'TxID': f"COST_{idx}_{date_val.strftime('%Y%m')}",
                    'Case_No': case_no,
                    'Date': date_val,
                    'Loc_From': None,
                    'Loc_To': location,
                    'Site': map_site(site_col=row.get('Site')),
                    'Qty': 0,  # ë¹„ìš©ì€ ìˆ˜ëŸ‰ ì—†ìŒ
                    'SQM': 0,
                    'CBM': 0,
                    'Cost': cost_val,
                    'TxType': 'COST',
                    'SOURCE_FILE': os.path.basename(filepath),
                    'FILE_TYPE': 'INVOICE'
                })
            
            print(f"   âœ… Invoice íŒŒì¼ ë¡œë”© ì™„ë£Œ: {len(movements)}ê±´")
            return movements
            
        except Exception as e:
            print(f"   âŒ Invoice íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return []
    
    @staticmethod
    def load_onhand_snapshot(filepath: str) -> List[Dict]:
        """OnHand ì¬ê³  ìŠ¤ëƒ…ìƒ· ë¡œë”©"""
        try:
            xl_file = pd.ExcelFile(filepath)
            
            # OnHand ì‹œíŠ¸ ì°¾ê¸°
            sheet_name = xl_file.sheet_names[0]
            for sheet in xl_file.sheet_names:
                if any(keyword in sheet.lower() for keyword in ['onhand', 'stock', 'inventory']):
                    sheet_name = sheet
                    break
            
            df = pd.read_excel(filepath, sheet_name=sheet_name)
            
            # ì»¬ëŸ¼ ë§¤í•‘
            loc_col = fuzzy_find_column(df, ['location', 'warehouse', 'loc', 'place'])
            qty_col = fuzzy_find_column(df, ['quantity', 'qty', 'stock', 'count'])
            case_col = fuzzy_find_column(df, ['case', 'case no', 'item'])
            
            if not qty_col:
                print(f"   âš ï¸ Quantity ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
                return []
            
            movements = []
            snapshot_date = datetime.now()
            
            for idx, row in df.iterrows():
                location = "UNKNOWN"
                if loc_col and pd.notna(row[loc_col]):
                    location = map_loc(row[loc_col])
                
                qty_val = pd.to_numeric(row[qty_col], errors='coerce') or 0
                
                case_no = f"SNAP_{idx}_{os.path.basename(filepath)}"
                if case_col and pd.notna(row[case_col]):
                    case_no = str(row[case_col])
                
                movements.append({
                    'TxID': f"SNAP_{idx}_{snapshot_date.strftime('%Y%m%d')}",
                    'Case_No': case_no,
                    'Date': snapshot_date,
                    'Loc_From': None,
                    'Loc_To': location,
                    'Site': "UNK",
                    'Qty': qty_val,
                    'SQM': 0,
                    'CBM': 0,
                    'Cost': 0,
                    'TxType': 'SNAP',
                    'SOURCE_FILE': os.path.basename(filepath),
                    'FILE_TYPE': 'ONHAND'
                })
            
            print(f"   âœ… OnHand íŒŒì¼ ë¡œë”© ì™„ë£Œ: {len(movements)}ê±´")
            return movements
            
        except Exception as e:
            print(f"   âŒ OnHand íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return []

# =============================================================================
# 3. CORRECTED STOCK ENGINE - ìˆ˜ì •ëœ ì¬ê³  ê³„ì‚° ì—”ì§„
# =============================================================================

class StockEngine:
    """ìˆ˜ì •ëœ HVDC ì¬ê³  ê³„ì‚° ì—”ì§„"""
    
    @staticmethod
    def _expand_transfer(tx: pd.DataFrame) -> pd.DataFrame:
        """TxType='TRANSFER' 1í–‰ â†’ OUT+IN 2í–‰ ë¶„ë¦¬"""
        if tx.empty:
            return tx
            
        transfer_mask = tx['TxType'].str.upper() == 'TRANSFER'
        transfers = tx[transfer_mask].copy()
        
        if transfers.empty:
            return tx
        
        # OUT ê¸°ë¡ (Loc_Fromì—ì„œ ë‚˜ê°)
        out_records = transfers.copy()
        out_records['TxType'] = 'OUT'
        out_records['Loc_To'] = pd.NA
        
        # IN ê¸°ë¡ (Loc_Toë¡œ ë“¤ì–´ê°)
        in_records = transfers.copy()
        in_records['TxType'] = 'IN'  
        in_records['Loc_From'] = pd.NA
        
        # ì›ë³¸ì—ì„œ TRANSFER ì œê±°í•˜ê³  ë¶„í•´ëœ ê¸°ë¡ ì¶”ê°€
        non_transfers = tx[~transfer_mask].copy()
        
        return pd.concat([non_transfers, out_records, in_records], ignore_index=True)
    
    @staticmethod
    def stock_daily(tx: pd.DataFrame) -> pd.DataFrame:
        """
        ì¼ë³„ ì¬ê³  ê³„ì‚°: Opening/Inbound/Outbound/Closing
        """
        if tx.empty:
            return pd.DataFrame()
        
        # TRANSFER ë¶„í•´
        tx_expanded = StockEngine._expand_transfer(tx).copy()
        
        # ìœ„ì¹˜ ì •ë³´ ì •ë¦¬ (INì€ Loc_To, OUTì€ Loc_From)
        tx_expanded['Loc'] = np.where(
            tx_expanded['TxType'].str.upper() == 'IN',
            tx_expanded['Loc_To'],
            tx_expanded['Loc_From']
        )
        
        # ì…ì¶œê³  êµ¬ë¶„
        tx_expanded['Inbound'] = np.where(
            tx_expanded['TxType'].str.upper() == 'IN',
            tx_expanded['Qty'], 0
        )
        tx_expanded['Outbound'] = np.where(
            tx_expanded['TxType'].str.upper() == 'OUT', 
            tx_expanded['Qty'], 0
        )
        
        # ë‚ ì§œë³„ ì§‘ê³„
        tx_expanded['Date'] = pd.to_datetime(tx_expanded['Date']).dt.date
        daily = tx_expanded.groupby(['Loc', 'Date'], as_index=False).agg({
            'Inbound': 'sum',
            'Outbound': 'sum',
            'SQM': 'sum'
        })
        
        # ê° ìœ„ì¹˜ë³„ë¡œ ëˆ„ì  ê³„ì‚°
        snapshots = []
        for loc in daily['Loc'].unique():
            if pd.isna(loc) or loc == 'UNKNOWN':
                continue
                
            loc_data = daily[daily['Loc'] == loc].copy()
            loc_data = loc_data.set_index('Date').sort_index()
            
            # ìˆ˜ì •ëœ ì¬ê³  ê³„ì‚°: Opening = ì „ì¼ Closing
            loc_data['Opening'] = 0  # ì´ˆê¸°ê°’
            loc_data['Closing'] = 0  # ì´ˆê¸°ê°’
            
            for i in range(len(loc_data)):
                if i == 0:
                    # ì²« ë²ˆì§¸ ë‚ : Opening = 0, Closing = Inbound - Outbound
                    loc_data.iloc[i, loc_data.columns.get_loc('Opening')] = 0
                    loc_data.iloc[i, loc_data.columns.get_loc('Closing')] = (
                        loc_data.iloc[i]['Inbound'] - loc_data.iloc[i]['Outbound']
                    )
                else:
                    # ì´í›„ ë‚ ë“¤: Opening = ì „ì¼ Closing, Closing = Opening + Inbound - Outbound
                    prev_closing = loc_data.iloc[i-1]['Closing']
                    loc_data.iloc[i, loc_data.columns.get_loc('Opening')] = prev_closing
                    loc_data.iloc[i, loc_data.columns.get_loc('Closing')] = (
                        prev_closing + loc_data.iloc[i]['Inbound'] - loc_data.iloc[i]['Outbound']
                    )
            
            loc_data['Loc'] = loc
            
            snapshots.append(loc_data.reset_index().rename(columns={'index': 'Date'}))
        
        if snapshots:
            result = pd.concat(snapshots, ignore_index=True)
            return result[['Loc', 'Date', 'Opening', 'Inbound', 'Outbound', 'Closing', 'SQM']].sort_values(['Loc', 'Date'])
        else:
            return pd.DataFrame()
    
    @staticmethod
    def create_proper_monthly_warehouse_analysis(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """ì˜¬ë°”ë¥¸ ì°½ê³ ë³„ ì›”ë³„ ì…ì¶œê³  ì¬ê³  ë¶„ì„"""
        
        if df.empty:
            return {}
        
        # 1. íŠ¸ëœì­ì…˜ ì •ê·œí™” (ê° ì´ë²¤íŠ¸ë¥¼ ë³„ë„ í–‰ìœ¼ë¡œ)
        transactions = []
        
        for _, row in df.iterrows():
            case_no = row['Case_No']
            qty = row['Qty']
            sqm = row['SQM']
            
            # ì°½ê³  ì…ê³  ì´ë²¤íŠ¸ë“¤
            if pd.notna(row.get('Loc_To')):
                transactions.append({
                    'Case_No': case_no,
                    'Date': pd.to_datetime(row['Date']),
                    'YearMonth': pd.to_datetime(row['Date']).strftime('%Y-%m'),
                    'Location': row['Loc_To'],
                    'TxType': 'IN',
                    'Qty': qty,
                    'SQM': sqm
                })
            
            # ì°½ê³  ì¶œê³  ì´ë²¤íŠ¸ë“¤ (ì‚¬ì´íŠ¸ë¡œ ë°°ì†¡)
            if pd.notna(row.get('Site')) and row['TxType'] == 'OUT':
                # ì¶œê³ ëŠ” ë§ˆì§€ë§‰ ì°½ê³ ì—ì„œ ë°œìƒ
                last_warehouse = row.get('Loc_From', 'UNKNOWN')
                transactions.append({
                    'Case_No': case_no,
                    'Date': pd.to_datetime(row['Date']),
                    'YearMonth': pd.to_datetime(row['Date']).strftime('%Y-%m'),
                    'Location': last_warehouse,
                    'TxType': 'OUT',
                    'Qty': qty,
                    'SQM': sqm
                })
        
        tx_df = pd.DataFrame(transactions)
        
        if tx_df.empty:
            return {}
        
        # 2. ì›”ë³„ ì°½ê³ ë³„ ì…ì¶œê³  ì§‘ê³„
        monthly_summary = tx_df.groupby(['Location', 'YearMonth', 'TxType']).agg({
            'Qty': 'sum',
            'SQM': 'sum',
            'Case_No': 'nunique'
        }).reset_index()
        
        # 3. ì…ê³ /ì¶œê³  ë¶„ë¦¬
        inbound = monthly_summary[monthly_summary['TxType'] == 'IN'].copy()
        outbound = monthly_summary[monthly_summary['TxType'] == 'OUT'].copy()
        
        # 4. ì „ì²´ ì›” ë²”ìœ„ ìƒì„± (ë¹ˆ ì›” 0ìœ¼ë¡œ ì±„ìš°ê¸°)
        all_months = sorted(tx_df['YearMonth'].unique())
        all_locations = sorted(tx_df['Location'].unique())
        
        # 5. ê° ì°½ê³ ë³„ë¡œ ì›”ë³„ ì¬ê³  ê³„ì‚°
        stock_results = []
        
        for location in all_locations:
            if location == 'UNKNOWN':
                continue
                
            location_stock = []
            opening_stock = 0  # ì´ˆê¸° ì¬ê³ 
            
            for month in all_months:
                # í•´ë‹¹ ì›” ì…ê³ ëŸ‰
                month_inbound = inbound[
                    (inbound['Location'] == location) & 
                    (inbound['YearMonth'] == month)
                ]['Qty'].sum()
                
                # í•´ë‹¹ ì›” ì¶œê³ ëŸ‰
                month_outbound = outbound[
                    (outbound['Location'] == location) & 
                    (outbound['YearMonth'] == month)
                ]['Qty'].sum()
                
                # í•´ë‹¹ ì›” ì…ê³  ë©´ì 
                month_inbound_sqm = inbound[
                    (inbound['Location'] == location) & 
                    (inbound['YearMonth'] == month)
                ]['SQM'].sum()
                
                # í•´ë‹¹ ì›” ì¶œê³  ë©´ì 
                month_outbound_sqm = outbound[
                    (outbound['Location'] == location) & 
                    (outbound['YearMonth'] == month)
                ]['SQM'].sum()
                
                # ì¬ê³  ê³„ì‚°
                closing_stock = opening_stock + month_inbound - month_outbound
                
                location_stock.append({
                    'Location': location,
                    'YearMonth': month,
                    'Opening_Stock': opening_stock,
                    'Inbound_Qty': month_inbound,
                    'Outbound_Qty': month_outbound,
                    'Closing_Stock': closing_stock,
                    'Inbound_SQM': month_inbound_sqm,
                    'Outbound_SQM': month_outbound_sqm,
                    'Net_Movement': month_inbound - month_outbound
                })
                
                # ë‹¤ìŒ ì›”ì˜ openingì€ ì´ë²ˆ ì›”ì˜ closing
                opening_stock = closing_stock
            
            stock_results.extend(location_stock)
        
        stock_df = pd.DataFrame(stock_results)
        
        # 6. í”¼ë²— í…Œì´ë¸”ë“¤ ìƒì„±
        inbound_pivot = stock_df.pivot_table(
            index='Location', 
            columns='YearMonth', 
            values='Inbound_Qty', 
            fill_value=0
        ).reset_index()
        
        outbound_pivot = stock_df.pivot_table(
            index='Location', 
            columns='YearMonth', 
            values='Outbound_Qty', 
            fill_value=0
        ).reset_index()
        
        closing_stock_pivot = stock_df.pivot_table(
            index='Location', 
            columns='YearMonth', 
            values='Closing_Stock', 
            fill_value=0
        ).reset_index()
        
        # 7. ëˆ„ì  í†µê³„
        cumulative_inbound = inbound_pivot.set_index('Location').cumsum(axis=1).reset_index()
        cumulative_outbound = outbound_pivot.set_index('Location').cumsum(axis=1).reset_index()
        
        return {
            'monthly_stock_detail': stock_df,
            'monthly_inbound_pivot': inbound_pivot,
            'monthly_outbound_pivot': outbound_pivot,
            'monthly_closing_stock_pivot': closing_stock_pivot,
            'cumulative_inbound': cumulative_inbound,
            'cumulative_outbound': cumulative_outbound,
            'monthly_summary_raw': monthly_summary
        }
    
    @staticmethod
    def validate_stock_logic(monthly_results: Dict) -> Dict[str, Any]:
        """ì¬ê³  ë¡œì§ ê²€ì¦"""
        
        if 'monthly_stock_detail' not in monthly_results:
            return {'validation_passed': False, 'errors': ['No stock detail data']}
        
        stock_df = monthly_results['monthly_stock_detail']
        errors = []
        warnings = []
        
        # 1. ì¬ê³  ë¬´ê²°ì„± ê²€ì¦
        for _, row in stock_df.iterrows():
            expected_closing = row['Opening_Stock'] + row['Inbound_Qty'] - row['Outbound_Qty']
            if abs(row['Closing_Stock'] - expected_closing) > 0.01:
                errors.append(f"Stock mismatch for {row['Location']} {row['YearMonth']}: "
                            f"Expected {expected_closing}, Got {row['Closing_Stock']}")
        
        # 2. ì—°ì†ì„± ê²€ì¦ (ë‹¤ìŒ ì›” Opening = ì´ì „ ì›” Closing)
        for location in stock_df['Location'].unique():
            loc_data = stock_df[stock_df['Location'] == location].sort_values('YearMonth')
            
            for i in range(1, len(loc_data)):
                prev_closing = loc_data.iloc[i-1]['Closing_Stock']
                curr_opening = loc_data.iloc[i]['Opening_Stock']
                
                if abs(prev_closing - curr_opening) > 0.01:
                    errors.append(f"Continuity error for {location}: "
                                f"Month {loc_data.iloc[i-1]['YearMonth']} closing {prev_closing} "
                                f"!= Month {loc_data.iloc[i]['YearMonth']} opening {curr_opening}")
        
        # 3. ìŒìˆ˜ ì¬ê³  ê²½ê³ 
        negative_stock = stock_df[stock_df['Closing_Stock'] < 0]
        if not negative_stock.empty:
            warnings.append(f"Found {len(negative_stock)} instances of negative stock")
        
        # 4. ì…ì¶œê³  ê· í˜• ê²€ì¦
        for location in stock_df['Location'].unique():
            loc_data = stock_df[stock_df['Location'] == location]
            total_inbound = loc_data['Inbound_Qty'].sum()
            total_outbound = loc_data['Outbound_Qty'].sum()
            final_stock = loc_data['Closing_Stock'].iloc[-1]
            
            # ì´ ì…ê³  - ì´ ì¶œê³  = ìµœì¢… ì¬ê³  (ì´ˆê¸° ì¬ê³ ê°€ 0ì´ë¼ê³  ê°€ì •)
            expected_final = total_inbound - total_outbound
            if abs(final_stock - expected_final) > 0.01:
                errors.append(f"Balance error for {location}: "
                            f"Total In({total_inbound}) - Total Out({total_outbound}) "
                            f"= {expected_final}, but final stock is {final_stock}")
        
        return {
            'validation_passed': len(errors) == 0,
            'errors': errors,
            'warnings': warnings,
            'total_records_checked': len(stock_df),
            'locations_checked': stock_df['Location'].nunique(),
            'months_checked': stock_df['YearMonth'].nunique()
        }
    
    @staticmethod
    def stock_monthly_site(tx: pd.DataFrame) -> pd.DataFrame:
        """ì›”ë³„ ì°½ê³ Â·í˜„ì¥ ì§‘ê³„ (BoxÂ·SQM)"""
        if tx.empty:
            return pd.DataFrame()
        
        # ê° ì¼€ì´ìŠ¤ì˜ ìµœì¢… ìƒíƒœë§Œ ì‚¬ìš© (ë§ˆì§€ë§‰ íŠ¸ëœì­ì…˜)
        tx_sorted = tx.sort_values('Date')
        latest_snapshot = tx_sorted.groupby('Case_No').tail(1).copy()
        
        # ì›” ì •ë³´ ì¶”ê°€
        latest_snapshot['YearMonth'] = pd.to_datetime(latest_snapshot['Date']).dt.to_period('M').astype(str)
        
        # ì›”ë³„ ì§‘ê³„
        monthly = latest_snapshot.groupby(['Loc_To', 'Site', 'YearMonth'], as_index=False).agg({
            'Case_No': 'nunique',  # BoxQty
            'Qty': 'sum',         # ì´ ìˆ˜ëŸ‰
            'SQM': 'sum',         # ì´ ë©´ì 
            'CBM': 'sum'          # ì´ ë¶€í”¼
        }).rename(columns={
            'Case_No': 'BoxQty',
            'Loc_To': 'Loc'
        })
        
        return monthly.sort_values(['YearMonth', 'Loc', 'Site'])
    
    @staticmethod
    def reconcile(daily_stock: pd.DataFrame, onhand_snap: pd.DataFrame) -> pd.DataFrame:
        """Tx ê¸°ë°˜ Closing vs OnHand ìŠ¤ëƒ…ìƒ· ì°¨ì´ ë¶„ì„"""
        if daily_stock.empty or onhand_snap.empty:
            return pd.DataFrame()
        
        # íŠ¸ëœì­ì…˜ ê¸°ë¡ì˜ ìµœì¢… ì¬ê³ 
        tx_last = daily_stock.sort_values('Date').groupby('Loc').tail(1).set_index('Loc')['Closing']
        
        # ì‹¤ì œ ì¬ê³  ìŠ¤ëƒ…ìƒ·  
        onhand_df = pd.DataFrame(onhand_snap)
        snap_sum = onhand_df.groupby('Loc_To')['Qty'].sum()
        
        # ë¹„êµ
        comparison = pd.concat([tx_last, snap_sum], axis=1, keys=['Tx_Closing', 'OnHand_Qty']).fillna(0)
        comparison['Î”'] = comparison['OnHand_Qty'] - comparison['Tx_Closing']
        comparison['Status'] = np.where(
            abs(comparison['Î”']) <= 5, 
            "OK", 
            "ATTENTION"
        )
        comparison['Alert_Level'] = np.where(
            abs(comparison['Î”']) > 10,
            "HIGH",
            np.where(abs(comparison['Î”']) > 5, "MEDIUM", "LOW")
        )
        
        return comparison.rename_axis('Loc').reset_index()

# =============================================================================
# 4. ADVANCED ANALYTICS - ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥
# =============================================================================

class AdvancedAnalytics:
    """ê³ ê¸‰ ë¶„ì„ ë° KPI ê³„ì‚°"""
    
    @staticmethod
    def create_warehouse_monthly_analysis(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """ì°½ê³ ë³„ ì›”ë³„ ìƒì„¸ ë¶„ì„ - ìˆ˜ì •ëœ ë¡œì§ ì‚¬ìš©"""
        # ìˆ˜ì •ëœ StockEngineì˜ ì˜¬ë°”ë¥¸ ì›”ë³„ ë¶„ì„ ë¡œì§ ì‚¬ìš©
        return StockEngine.create_proper_monthly_warehouse_analysis(df)
    
    @staticmethod
    def create_site_delivery_analysis(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """í˜„ì¥ë³„ ë°°ì†¡ ìƒì„¸ ë¶„ì„"""
        if df.empty:
            return {}
        
        site_df = df[df['TxType'] == 'OUT'].copy()
        site_df['YearMonth'] = pd.to_datetime(site_df['Date']).dt.to_period('M').astype(str)
        
        # ì›”ë³„ í˜„ì¥ ë°°ì†¡
        monthly_delivery = site_df.groupby(['Site', 'YearMonth']).agg({
            'Qty': 'sum',
            'SQM': 'sum',
            'CBM': 'sum',
            'Case_No': 'nunique'
        }).reset_index()
        
        # í”¼ë²— í…Œì´ë¸”ë“¤
        delivery_pivot_qty = monthly_delivery.pivot_table(
            index='Site', columns='YearMonth', values='Qty', fill_value=0
        ).reset_index()
        
        delivery_pivot_sqm = monthly_delivery.pivot_table(
            index='Site', columns='YearMonth', values='SQM', fill_value=0
        ).reset_index()
        
        # ëˆ„ì  ë°°ì†¡ëŸ‰
        cumulative_delivery = delivery_pivot_qty.set_index('Site').cumsum(axis=1).reset_index()
        
        return {
            'monthly_delivery_summary': monthly_delivery,
            'delivery_pivot_qty': delivery_pivot_qty,
            'delivery_pivot_sqm': delivery_pivot_sqm,
            'cumulative_delivery': cumulative_delivery
        }
    
    @staticmethod
    def create_integrated_flow_analysis(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """í†µí•© ì°½ê³ -í˜„ì¥ íë¦„ ë¶„ì„"""
        if df.empty:
            return {}
        
        df['YearMonth'] = pd.to_datetime(df['Date']).dt.to_period('M').astype(str)
        
        # ì¼€ì´ìŠ¤ë³„ ì´ë™ ê²½ë¡œ ì¶”ì 
        warehouse_to_site_flows = []
        
        for case_no in df['Case_No'].unique():
            case_movements = df[df['Case_No'] == case_no].sort_values('Date')
            
            warehouse_visits = case_movements[case_movements['TxType'] == 'IN']
            site_deliveries = case_movements[case_movements['TxType'] == 'OUT']
            
            if not warehouse_visits.empty and not site_deliveries.empty:
                last_warehouse = warehouse_visits.iloc[-1]['Loc_To']
                final_site = site_deliveries.iloc[-1]['Site']
                final_month = site_deliveries.iloc[-1]['YearMonth']
                qty = case_movements.iloc[0]['Qty']
                sqm = case_movements.iloc[0]['SQM']
                
                warehouse_to_site_flows.append({
                    'Source_Warehouse': last_warehouse,
                    'Destination_Site': final_site,
                    'YearMonth': final_month,
                    'Qty': qty,
                    'SQM': sqm,
                    'Case_No': case_no
                })
        
        if not warehouse_to_site_flows:
            return {}
        
        flow_df = pd.DataFrame(warehouse_to_site_flows)
        
        # ì°½ê³ â†’í˜„ì¥ íë¦„ ìš”ì•½
        flow_summary = flow_df.groupby(['Source_Warehouse', 'Destination_Site']).agg({
            'Qty': 'sum',
            'SQM': 'sum',
            'Case_No': 'nunique'
        }).reset_index().rename(columns={'Case_No': 'Total_Cases'})
        
        # ì›”ë³„ íë¦„ íŠ¸ë Œë“œ
        monthly_flow = flow_df.groupby(['YearMonth', 'Source_Warehouse', 'Destination_Site']).agg({
            'Qty': 'sum',
            'SQM': 'sum',
            'Case_No': 'nunique'
        }).reset_index().rename(columns={'Case_No': 'Cases'})
        
        # ì°½ê³ ë³„ íš¨ìœ¨ì„± ë¶„ì„
        warehouse_efficiency = flow_df.groupby('Source_Warehouse').agg({
            'Qty': ['sum', 'mean'],
            'SQM': ['sum', 'mean'],
            'Destination_Site': 'nunique',
            'Case_No': 'nunique'
        }).round(2)
        
        warehouse_efficiency.columns = ['Total_Qty', 'Avg_Qty_Per_Case', 'Total_SQM', 'Avg_SQM_Per_Case', 'Sites_Served', 'Total_Cases']
        warehouse_efficiency = warehouse_efficiency.reset_index()
        
        return {
            'flow_summary': flow_summary,
            'monthly_flow': monthly_flow,
            'warehouse_efficiency': warehouse_efficiency,
            'detailed_flows': flow_df
        }
    
    @staticmethod
    def create_cost_analysis(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """ë¹„ìš© ë¶„ì„"""
        cost_df = df[df['TxType'] == 'COST'].copy()
        
        if cost_df.empty:
            return {}
        
        cost_df['YearMonth'] = pd.to_datetime(cost_df['Date']).dt.to_period('M').astype(str)
        
        # ì›”ë³„ ìœ„ì¹˜ë³„ ë¹„ìš©
        monthly_cost = cost_df.groupby(['Loc_To', 'YearMonth']).agg({
            'Cost': 'sum'
        }).reset_index().rename(columns={'Loc_To': 'Location'})
        
        # ë¹„ìš© í”¼ë²— í…Œì´ë¸”
        cost_pivot = monthly_cost.pivot_table(
            index='Location', columns='YearMonth', values='Cost', fill_value=0
        ).reset_index()
        
        # ëˆ„ì  ë¹„ìš©
        cumulative_cost = cost_pivot.set_index('Location').cumsum(axis=1).reset_index()
        
        # ë¹„ìš© í†µê³„
        cost_stats = cost_df.groupby('Loc_To').agg({
            'Cost': ['sum', 'mean', 'min', 'max', 'std']
        }).round(2)
        
        cost_stats.columns = ['Total_Cost', 'Avg_Cost', 'Min_Cost', 'Max_Cost', 'Std_Cost']
        cost_stats = cost_stats.reset_index().rename(columns={'Loc_To': 'Location'})
        
        return {
            'monthly_cost': monthly_cost,
            'cost_pivot': cost_pivot,
            'cumulative_cost': cumulative_cost,
            'cost_statistics': cost_stats
        }
    
    @staticmethod
    def create_kpi_dashboard(df: pd.DataFrame, daily_stock: pd.DataFrame, reconcile_result: pd.DataFrame) -> Dict[str, Any]:
        """KPI ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""
        
        # ê¸°ë³¸ í†µê³„
        total_cases = df['Case_No'].nunique()
        total_qty = df['Qty'].sum()
        total_sqm = df['SQM'].sum()
        unique_warehouses = df['Loc_To'].nunique()
        unique_sites = df['Site'].nunique()
        
        # ë‚ ì§œ ë²”ìœ„
        date_range = f"{df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}"
        
        # ì¬ê³  ì •í™•ë„
        if not reconcile_result.empty:
            total_reconcile_items = len(reconcile_result)
            attention_items = len(reconcile_result[reconcile_result['Status'] == 'ATTENTION'])
            accuracy_rate = (total_reconcile_items - attention_items) / total_reconcile_items * 100 if total_reconcile_items > 0 else 100
        else:
            total_reconcile_items = 0
            attention_items = 0
            accuracy_rate = 100
        
        # ì›”ë³„ ì²˜ë¦¬ëŸ‰ íŠ¸ë Œë“œ
        df['YearMonth'] = pd.to_datetime(df['Date']).dt.to_period('M').astype(str)
        monthly_trend = df.groupby('YearMonth').agg({
            'Qty': 'sum',
            'SQM': 'sum',
            'Case_No': 'nunique'
        }).reset_index()
        
        # ì°½ê³ ë³„ ì²˜ë¦¬ëŸ‰
        warehouse_performance = df[df['TxType'].isin(['IN', 'OUT'])].groupby('Loc_To').agg({
            'Qty': 'sum',
            'SQM': 'sum',
            'Case_No': 'nunique'
        }).reset_index().rename(columns={'Loc_To': 'Warehouse'})
        
        # í˜„ì¥ë³„ ë°°ì†¡ ì‹¤ì 
        site_performance = df[df['TxType'] == 'OUT'].groupby('Site').agg({
            'Qty': 'sum',
            'SQM': 'sum',
            'Case_No': 'nunique'
        }).reset_index()
        
        return {
            'summary_stats': {
                'Total_Cases': total_cases,
                'Total_Quantity': total_qty,
                'Total_SQM': total_sqm,
                'Unique_Warehouses': unique_warehouses,
                'Unique_Sites': unique_sites,
                'Date_Range': date_range,
                'Stock_Accuracy_Rate': accuracy_rate,
                'Reconcile_Items': total_reconcile_items,
                'Attention_Items': attention_items,
                'Analysis_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            'monthly_trend': monthly_trend,
            'warehouse_performance': warehouse_performance,
            'site_performance': site_performance
        }

# =============================================================================
# 5. REPORT WRITER V2 - ê³ ê¸‰ Excel ë¦¬í¬íŠ¸ ìƒì„±
# =============================================================================

class ReportWriter:
    """ê³ ê¸‰ Excel ë¦¬í¬íŠ¸ ìƒì„±ê¸°"""
    
    @staticmethod
    def format_excel_sheet(workbook, worksheet, df: pd.DataFrame):
        """Excel ì‹œíŠ¸ ê³ ê¸‰ ì„œì‹ ì ìš©"""
        # í—¤ë” ì„œì‹
        header_format = workbook.add_format({
            'bold': True,
            'text_wrap': True,
            'valign': 'top',
            'fg_color': '#D7E4BC',
            'border': 1,
            'align': 'center'
        })
        
        # ìˆ«ì ì„œì‹
        number_format = workbook.add_format({'num_format': '#,##0.00'})
        currency_format = workbook.add_format({'num_format': '#,##0.00 "AED"'})
        percent_format = workbook.add_format({'num_format': '0.00%'})
        date_format = workbook.add_format({'num_format': 'yyyy-mm-dd'})
        
        # í—¤ë” ì ìš©
        for col_num, value in enumerate(df.columns.values):
            worksheet.write(0, col_num, value, header_format)
        
        # ì»¬ëŸ¼ë³„ ì„œì‹ ì ìš©
        for i, col in enumerate(df.columns):
            if col in ['Qty', 'SQM', 'CBM', 'BoxQty', 'Total_Cases', 'Cases']:
                worksheet.set_column(i, i, 15, number_format)
            elif col in ['Cost', 'Total_Cost', 'Avg_Cost']:
                worksheet.set_column(i, i, 15, currency_format)
            elif 'Rate' in col or 'Accuracy' in col:
                worksheet.set_column(i, i, 15, percent_format)
            elif 'Date' in col:
                worksheet.set_column(i, i, 15, date_format)
            else:
                col_width = max(df[col].astype(str).map(len).max(), len(str(col))) + 2
                worksheet.set_column(i, i, min(col_width, 30))
    
    @staticmethod
    def save_comprehensive_report(all_data: Dict, output_file: str = "HVDC_Comprehensive_Report.xlsx"):
        """ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥"""
        
        with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
            workbook = writer.book
            
            # 1. ì¢…í•© ëŒ€ì‹œë³´ë“œ
            if 'kpi_dashboard' in all_data:
                kpi = all_data['kpi_dashboard']
                
                # ìš”ì•½ í†µê³„
                if 'summary_stats' in kpi:
                    stats_data = [[k, v] for k, v in kpi['summary_stats'].items()]
                    stats_df = pd.DataFrame(stats_data, columns=['í•­ëª©', 'ê°’'])
                    stats_df.to_excel(writer, sheet_name='ğŸ“Š_Dashboard', index=False)
                    ReportWriter.format_excel_sheet(workbook, writer.sheets['ğŸ“Š_Dashboard'], stats_df)
            
            # 2. ì°½ê³ ë³„ ì›”ë³„ ìƒì„¸ ë¶„ì„
            if 'warehouse_monthly' in all_data:
                wm = all_data['warehouse_monthly']
                
                for key, df in wm.items():
                    if not df.empty:
                        sheet_name = f"ğŸ¢_{key}"[:31]  # Excel ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                        ReportWriter.format_excel_sheet(workbook, writer.sheets[sheet_name], df)
            
            # 3. í˜„ì¥ë³„ ë°°ì†¡ ë¶„ì„
            if 'site_delivery' in all_data:
                sd = all_data['site_delivery']
                
                for key, df in sd.items():
                    if not df.empty:
                        sheet_name = f"ğŸ—ï¸_{key}"[:31]
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                        ReportWriter.format_excel_sheet(workbook, writer.sheets[sheet_name], df)
            
            # 4. í†µí•© íë¦„ ë¶„ì„
            if 'integrated_flow' in all_data:
                flow = all_data['integrated_flow']
                
                for key, df in flow.items():
                    if not df.empty:
                        sheet_name = f"ğŸ”„_{key}"[:31]
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                        ReportWriter.format_excel_sheet(workbook, writer.sheets[sheet_name], df)
            
            # 5. ë¹„ìš© ë¶„ì„
            if 'cost_analysis' in all_data:
                cost = all_data['cost_analysis']
                
                for key, df in cost.items():
                    if not df.empty:
                        sheet_name = f"ğŸ’°_{key}"[:31]
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                        ReportWriter.format_excel_sheet(workbook, writer.sheets[sheet_name], df)
            
            # 6. ì¼ë³„ ì¬ê³  ì¶”ì 
            if 'daily_stock' in all_data and not all_data['daily_stock'].empty:
                all_data['daily_stock'].to_excel(writer, sheet_name='ğŸ“…_ì¼ë³„ì¬ê³ ì¶”ì ', index=False)
                ReportWriter.format_excel_sheet(workbook, writer.sheets['ğŸ“…_ì¼ë³„ì¬ê³ ì¶”ì '], all_data['daily_stock'])
            
            # 7. ì¬ê³  ì°¨ì´ ë¶„ì„
            if 'reconcile_result' in all_data and not all_data['reconcile_result'].empty:
                all_data['reconcile_result'].to_excel(writer, sheet_name='âš–ï¸_ì¬ê³ ì°¨ì´ë¶„ì„', index=False)
                ReportWriter.format_excel_sheet(workbook, writer.sheets['âš–ï¸_ì¬ê³ ì°¨ì´ë¶„ì„'], all_data['reconcile_result'])
            
            # 8. ì›ë³¸ ë°ì´í„°
            if 'raw_data' in all_data and not all_data['raw_data'].empty:
                all_data['raw_data'].to_excel(writer, sheet_name='ğŸ“„_ì›ë³¸ë°ì´í„°', index=False)
                ReportWriter.format_excel_sheet(workbook, writer.sheets['ğŸ“„_ì›ë³¸ë°ì´í„°'], all_data['raw_data'])
            
            # 9. íƒ€ì„ë¼ì¸ ë¶„ì„ ê²°ê³¼ (ì˜µì…˜)
            if 'timeline_transactions' in all_data and not all_data['timeline_transactions'].empty:
                all_data['timeline_transactions'].to_excel(writer, sheet_name='â³_Timeline_Transactions', index=False)
                ReportWriter.format_excel_sheet(workbook, writer.sheets['â³_Timeline_Transactions'], all_data['timeline_transactions'])
            
            if 'timeline_stock' in all_data and not all_data['timeline_stock'].empty:
                all_data['timeline_stock'].to_excel(writer, sheet_name='â³_Timeline_Stock', index=False)
                ReportWriter.format_excel_sheet(workbook, writer.sheets['â³_Timeline_Stock'], all_data['timeline_stock'])
            
            if 'timeline_validation' in all_data:
                # ê²€ì¦ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                validation = all_data['timeline_validation']
                validation_data = []
                validation_data.append(['ê²€ì¦ í•­ëª©', 'ê²°ê³¼', 'ìƒì„¸'])
                validation_data.append(['ì „ì²´ ê²€ì¦', 'í†µê³¼' if validation['validation_passed'] else 'ì‹¤íŒ¨', ''])
                validation_data.append(['ì˜¤ë¥˜ ìˆ˜', len(validation['errors']), ''])
                
                if 'warehouse_totals' in validation:
                    for warehouse, totals in validation['warehouse_totals'].items():
                        validation_data.append([
                            f'{warehouse} ì •í™•ë„',
                            f"{totals['accuracy']:.1f}%",
                            f"ì‹¤ì œ: {totals['actual']}, ê¸°ì¤€: {totals['expected']}"
                        ])
                
                validation_df = pd.DataFrame(validation_data[1:], columns=validation_data[0])
                validation_df.to_excel(writer, sheet_name='â³_Timeline_Validation', index=False)
                ReportWriter.format_excel_sheet(workbook, writer.sheets['â³_Timeline_Validation'], validation_df)
        
        print(f"âœ… ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {output_file}")
        
        # ìƒì„±ëœ ì‹œíŠ¸ ëª©ë¡ ì¶œë ¥
        print(f"\nğŸ“‹ ìƒì„±ëœ ì‹œíŠ¸ ëª©ë¡:")
        print(f"   ğŸ“Š Dashboard - ì¢…í•© KPI ëŒ€ì‹œë³´ë“œ")
        print(f"   ğŸ¢ Warehouse Monthly - ì°½ê³ ë³„ ì›”ë³„ ì…ì¶œê³  ë¶„ì„")
        print(f"   ğŸ—ï¸ Site Delivery - í˜„ì¥ë³„ ë°°ì†¡ ë¶„ì„")
        print(f"   ğŸ”„ Integrated Flow - ì°½ê³ â†”í˜„ì¥ íë¦„ ë¶„ì„")
        print(f"   ğŸ’° Cost Analysis - ë¹„ìš© ë¶„ì„")
        print(f"   ğŸ“… ì¼ë³„ì¬ê³ ì¶”ì  - ì¼ë³„ ìƒì„¸ ì¬ê³  ë³€ë™")
        print(f"   âš–ï¸ ì¬ê³ ì°¨ì´ë¶„ì„ - Tx vs OnHand ì°¨ì´")
        print(f"   ğŸ“„ ì›ë³¸ë°ì´í„° - ì „ì²´ íŠ¸ëœì­ì…˜ ë°ì´í„°")
        
        # íƒ€ì„ë¼ì¸ ì‹œíŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì¶œë ¥
        if 'timeline_transactions' in all_data:
            print(f"   â³ Timeline_Transactions - íƒ€ì„ë¼ì¸ íŠ¸ëœì­ì…˜")
            print(f"   â³ Timeline_Stock - íƒ€ì„ë¼ì¸ ì¬ê³  ê³„ì‚°")
            print(f"   â³ Timeline_Validation - íƒ€ì„ë¼ì¸ ê²€ì¦ ê²°ê³¼")

# =============================================================================
# 6. MAIN EXECUTION - ë©”ì¸ ì‹¤í–‰ ì—”ì§„
# =============================================================================

def find_hvdc_files() -> Dict[str, List[str]]:
    """í˜„ì¬ í´ë”ì—ì„œ HVDC íŒŒì¼ë“¤ ìë™ íƒì§€"""
    current_dir = os.getcwd()
    print(f"ğŸ“ í˜„ì¬ í´ë”: {current_dir}")
    
    files = {
        'warehouse': [],
        'invoice': [],
        'onhand': []
    }
    
    # data í´ë”ì—ì„œ xlsx íŒŒì¼ ìŠ¤ìº”
    data_dir = "data"
    if os.path.exists(data_dir):
        xlsx_files = glob.glob(os.path.join(data_dir, "*.xlsx"))
    else:
        # data í´ë”ê°€ ì—†ìœ¼ë©´ í˜„ì¬ í´ë”ì—ì„œ ìŠ¤ìº”
        xlsx_files = glob.glob("*.xlsx")
    
    for file in xlsx_files:
        file_type = detect_file_type(file)
        
        if file_type in ['BL', 'MIR', 'PICK']:
            files['warehouse'].append(file)
        elif file_type == 'INVOICE':
            files['invoice'].append(file)
        elif file_type == 'ONHAND':
            files['onhand'].append(file)
        elif 'HVDC' in file.upper() and 'WAREHOUSE' in file.upper():
            files['warehouse'].append(file)
        elif 'INVOICE' in file.upper():
            files['invoice'].append(file)
        elif 'ONHAND' in file.upper() or 'STOCK' in file.upper():
            files['onhand'].append(file)
    
    print("ğŸ” ë°œê²¬ëœ íŒŒì¼ë“¤:")
    for file_type, file_list in files.items():
        print(f"   {file_type}: {len(file_list)}ê°œ")
        for f in file_list:
            print(f"      - {f}")
    
    return files

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ HVDC Warehouse Ontology-based Analysis ì‹œì‘...")
    print("ğŸ“‹ End-to-End Pipeline: ìë™íƒì§€ â†’ ì •ê·œí™” â†’ ì¬ê³ ê³„ì‚° â†’ ê³ ê¸‰ë¶„ì„ â†’ ë¦¬í¬íŠ¸")
    
    # 1. íŒŒì¼ ìë™ íƒì§€
    files = find_hvdc_files()
    
    if not any(files.values()):
        print("âŒ HVDC íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•´ ì£¼ì„¸ìš”:")
        print("   - HVDCë¡œ ì‹œì‘í•˜ëŠ” .xlsx íŒŒì¼ì´ í˜„ì¬ í´ë”ì— ìˆëŠ”ì§€")
        print("   - íŒŒì¼ëª…ì´ ì •í™•í•œì§€ (ì˜ˆ: HVDC WAREHOUSE_HITACHI(HE).xlsx)")
        return
    
    # 2. ë°ì´í„° ë¡œë”© ë° ì •ê·œí™”
    print("\nğŸ“Š ë°ì´í„° ë¡œë”© ë° Ontology ë§¤í•‘ ì¤‘...")
    all_movements = []
    
    # ì°½ê³  íŒŒì¼ë“¤ ì²˜ë¦¬
    for wh_file in files['warehouse']:
        print(f"\nğŸ“‚ ì²˜ë¦¬ ì¤‘: {wh_file}")
        movements = DataExtractor.load_warehouse_file(wh_file)
        all_movements.extend(movements)
        print(f"   ğŸ“¦ ì¶”ì¶œëœ ì´ë™ ê¸°ë¡: {len(movements)}ê±´")
    
    # ì¸ë³´ì´ìŠ¤ íŒŒì¼ë“¤ ì²˜ë¦¬
    for inv_file in files['invoice']:
        print(f"\nğŸ’° ì²˜ë¦¬ ì¤‘: {inv_file}")
        movements = DataExtractor.load_invoice(inv_file)
        all_movements.extend(movements)
        print(f"   ğŸ’¸ ì¶”ì¶œëœ ë¹„ìš© ê¸°ë¡: {len(movements)}ê±´")
    
    # OnHand íŒŒì¼ë“¤ ì²˜ë¦¬
    onhand_movements = []
    for onhand_file in files['onhand']:
        print(f"\nğŸ“‹ ì²˜ë¦¬ ì¤‘: {onhand_file}")
        movements = DataExtractor.load_onhand_snapshot(onhand_file)
        onhand_movements.extend(movements)
        print(f"   ğŸ“Š ì¶”ì¶œëœ ì¬ê³  ìŠ¤ëƒ…ìƒ·: {len(movements)}ê±´")
    
    if not all_movements:
        print("âš ï¸ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 3. ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ê·œí™”
    print(f"\nğŸ“ˆ ë°ì´í„° ì •ê·œí™” ë° ë¶„ì„ ì¤‘...")
    df = pd.DataFrame(all_movements)
    onhand_df = pd.DataFrame(onhand_movements) if onhand_movements else pd.DataFrame()
    
    print(f"   ì´ íŠ¸ëœì­ì…˜ ê¸°ë¡: {len(df)}ê±´")
    print(f"   OnHand ìŠ¤ëƒ…ìƒ·: {len(onhand_df)}ê±´")
    
    # 4. í•µì‹¬ ì¬ê³  ê³„ì‚°
    print(f"\nğŸ”¢ ì¬ê³  ê³„ì‚° ì—”ì§„ ì‹¤í–‰ ì¤‘...")
    
    # ì¼ë³„ ì¬ê³ 
    daily_stock = StockEngine.stock_daily(df)
    print(f"   ì¼ë³„ ì¬ê³  í¬ì¸íŠ¸: {len(daily_stock)}ê°œ")
    
    # ì›”ë³„ ì‚¬ì´íŠ¸ ìš”ì•½
    monthly_site = StockEngine.stock_monthly_site(df)
    print(f"   ì›”ë³„ ì‚¬ì´íŠ¸ ìš”ì•½: {len(monthly_site)}ê°œ")
    
    # ì¬ê³  ì°¨ì´ ë¶„ì„
    reconcile_result = StockEngine.reconcile(daily_stock, onhand_movements) if onhand_movements else pd.DataFrame()
    if not reconcile_result.empty:
        attention_items = len(reconcile_result[reconcile_result['Status'] == 'ATTENTION'])
        print(f"   ì¬ê³  ì°¨ì´ ë¶„ì„: {len(reconcile_result)}ê°œ ìœ„ì¹˜ (ì£¼ì˜ í•„ìš”: {attention_items}ê°œ)")
    
    # 5. ê³ ê¸‰ ë¶„ì„ ì‹¤í–‰
    print(f"\nğŸ“Š ê³ ê¸‰ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
    
    # ì°½ê³ ë³„ ì›”ë³„ ë¶„ì„
    warehouse_monthly = AdvancedAnalytics.create_warehouse_monthly_analysis(df)
    print(f"   ì°½ê³ ë³„ ì›”ë³„ ë¶„ì„ ì™„ë£Œ")
    
    # ìˆ˜ì •ëœ ë¡œì§ ê²€ì¦
    print(f"\nğŸ” ìˆ˜ì •ëœ ì¬ê³  ë¡œì§ ê²€ì¦ ì¤‘...")
    validation_result = StockEngine.validate_stock_logic(warehouse_monthly)
    
    if validation_result['validation_passed']:
        print(f"   âœ… ì¬ê³  ë¡œì§ ê²€ì¦ í†µê³¼")
    else:
        print(f"   âš ï¸ ì¬ê³  ë¡œì§ ê²€ì¦ ì‹¤íŒ¨ - {len(validation_result['errors'])}ê°œ ì˜¤ë¥˜ ë°œê²¬")
        if validation_result['errors']:
            print(f"   ì²« ë²ˆì§¸ ì˜¤ë¥˜: {validation_result['errors'][0]}")
    
    if validation_result['warnings']:
        print(f"   âš ï¸ ê²½ê³ ì‚¬í•­: {len(validation_result['warnings'])}ê°œ")
        for warning in validation_result['warnings']:
            print(f"     - {warning}")
    
    print(f"   ğŸ“‹ ê²€ì¦ ëŒ€ìƒ: {validation_result['total_records_checked']}ê°œ ê¸°ë¡, {validation_result['locations_checked']}ê°œ ì°½ê³ , {validation_result['months_checked']}ê°œ ì›”")
    
    # í˜„ì¥ë³„ ë°°ì†¡ ë¶„ì„
    site_delivery = AdvancedAnalytics.create_site_delivery_analysis(df)
    print(f"   í˜„ì¥ë³„ ë°°ì†¡ ë¶„ì„ ì™„ë£Œ")
    
    # í†µí•© íë¦„ ë¶„ì„
    integrated_flow = AdvancedAnalytics.create_integrated_flow_analysis(df)
    print(f"   í†µí•© íë¦„ ë¶„ì„ ì™„ë£Œ")
    
    # ë¹„ìš© ë¶„ì„
    cost_analysis = AdvancedAnalytics.create_cost_analysis(df)
    print(f"   ë¹„ìš© ë¶„ì„ ì™„ë£Œ")
    
    # KPI ëŒ€ì‹œë³´ë“œ
    kpi_dashboard = AdvancedAnalytics.create_kpi_dashboard(df, daily_stock, reconcile_result)
    print(f"   KPI ëŒ€ì‹œë³´ë“œ ìƒì„± ì™„ë£Œ")
    
    # 6. íƒ€ì„ë¼ì¸ ê¸°ë°˜ ë¶„ì„ (ì˜µì…˜)
    timeline_results = None
    if TIMELINE_AVAILABLE:
        print(f"\nâ³ Timeline ê¸°ë°˜ ê¸°ì¤€ ìˆ˜ëŸ‰ ê²€ì¦ ì‹œì‘...")
        
        # HITACHI INDOOR ê°•ì œ ì¬ê³  íŒŒì¼ ì°¾ê¸°
        indoor_override_cases = set()
        indoor_file_path = None
        
        # data í´ë”ì—ì„œ HITACHI INDOOR íŒŒì¼ ì°¾ê¸°
        if os.path.exists("data"):
            for file in os.listdir("data"):
                if "HITACHI" in file.upper() and "LOCAL" in file.upper() and file.endswith(".xlsx"):
                    indoor_file_path = os.path.join("data", file)
                    break
        
        if indoor_file_path and os.path.exists(indoor_file_path):
            print(f"ğŸ  HITACHI INDOOR íŒŒì¼ ë°œê²¬: {indoor_file_path}")
            indoor_override_cases = ttm.load_indoor_stock_cases(indoor_file_path)
        
        # ê¸°ì¤€ ìˆ˜ëŸ‰ ì„¤ì •
        expected_totals = {
            'DSV Al Markaz': 812,
            'DSV Indoor': 414
        }
        
        # timeline_tracking_moduleì´ ê¸°ëŒ€í•˜ëŠ” ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€í™˜
        timeline_df = df.copy()
        rename_map = {}
        for col in ['Event_Type', 'Location', 'Source_File']:
            if col not in timeline_df.columns:
                # ì¶”ì • ë§¤í•‘
                if col == 'Event_Type':
                    for cand in ['TxType', 'TxType_Refined', 'EVENT_TYPE']:
                        if cand in timeline_df.columns:
                            rename_map[cand] = 'Event_Type'
                            break
                elif col == 'Location':
                    for cand in ['Loc_To', 'LOC_TO', 'Warehouse', 'LOC_FROM']:
                        if cand in timeline_df.columns:
                            rename_map[cand] = 'Location'
                            break
                elif col == 'Source_File':
                    for cand in ['SOURCE_FILE', 'Source', 'source']:
                        if cand in timeline_df.columns:
                            rename_map[cand] = 'Source_File'
                            break
        if rename_map:
            timeline_df = timeline_df.rename(columns=rename_map)
        # timeline ë¶„ì„ ì‹¤í–‰
        try:
            timeline_results = ttm.run_timeline_analysis(
                timeline_df, 
                indoor_override_cases=indoor_override_cases,
                expected_totals=expected_totals
            )
            print(f"âœ… Timeline ë¶„ì„ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Timeline ë¶„ì„ ì‹¤íŒ¨: {e}")
            timeline_results = None
    
    # 7. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    print(f"\nğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    all_analysis_data = {
        'raw_data': df,
        'daily_stock': daily_stock,
        'monthly_site': monthly_site,
        'reconcile_result': reconcile_result,
        'warehouse_monthly': warehouse_monthly,
        'site_delivery': site_delivery,
        'integrated_flow': integrated_flow,
        'cost_analysis': cost_analysis,
        'kpi_dashboard': kpi_dashboard,
        'onhand_data': onhand_df
    }
    
    # íƒ€ì„ë¼ì¸ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if timeline_results:
        all_analysis_data.update({
            'timeline_transactions': timeline_results['transactions'],
            'timeline_stock': timeline_results['stock_df'],
            'timeline_validation': timeline_results['validation'],
            'timeline_stats': timeline_results['stats']
        })
    
    # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    print("\nğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = f"HVDC_Comprehensive_Report_{timestamp}.xlsx"
    ReportWriter.save_comprehensive_report(all_analysis_data, report_filename)
    print(f"âœ… ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_filename}")
    
    # 8. ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print(f"\nğŸ“‹ ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
    
    if 'summary_stats' in kpi_dashboard:
        stats = kpi_dashboard['summary_stats']
        print(f"   ì´ ì²˜ë¦¬ ì¼€ì´ìŠ¤: {stats['Total_Cases']:,}ê°œ")
        print(f"   ì´ ìˆ˜ëŸ‰: {stats['Total_Quantity']:,}ë°•ìŠ¤")
        print(f"   ì´ ë©´ì : {stats['Total_SQM']:,.2f} SQM")
        print(f"   ì°½ê³  ìˆ˜: {stats['Unique_Warehouses']}ê°œ")
        print(f"   í˜„ì¥ ìˆ˜: {stats['Unique_Sites']}ê°œ")
        print(f"   ì¬ê³  ì •í™•ë„: {stats['Stock_Accuracy_Rate']:.1f}%")
    
    # íƒ€ì„ë¼ì¸ ê²°ê³¼ ì¶œë ¥
    if timeline_results:
        print(f"\nâ³ Timeline ë¶„ì„ ê²°ê³¼:")
        timeline_stats = timeline_results['stats']
        print(f"   ì´ ì¼€ì´ìŠ¤: {timeline_stats['total_cases']:,}ê°œ")
        print(f"   ì´ë™ ì¼€ì´ìŠ¤: {timeline_stats['transfer_cases']:,}ê°œ")
        print(f"   ì§ì ‘ Indoor: {timeline_stats['direct_indoor']:,}ê°œ")
        print(f"   ì§ì ‘ Al Markaz: {timeline_stats['direct_almarkaz']:,}ê°œ")
        print(f"   ë°°ì†¡ ì™„ë£Œ: {timeline_stats['delivered_cases']:,}ê°œ")
        
        # ê¸°ì¤€ ìˆ˜ëŸ‰ ë‹¬ì„± ì—¬ë¶€
        validation = timeline_results['validation']
        if validation['validation_passed']:
            print(f"   ğŸ¯ ê¸°ì¤€ ìˆ˜ëŸ‰ ë‹¬ì„±: âœ…")
        else:
            print(f"   ğŸ¯ ê¸°ì¤€ ìˆ˜ëŸ‰ ë‹¬ì„±: âŒ")
            for error in validation['errors'][:3]:  # ì²˜ìŒ 3ê°œ ì˜¤ë¥˜ë§Œ í‘œì‹œ
                print(f"     - {error}")
    
    # ì°½ê³ ë³„ ì„±ê³¼
    if 'warehouse_performance' in kpi_dashboard and not kpi_dashboard['warehouse_performance'].empty:
        print(f"\nğŸ¢ ì°½ê³ ë³„ ì²˜ë¦¬ í˜„í™©:")
        for _, row in kpi_dashboard['warehouse_performance'].head().iterrows():
            print(f"     {row['Warehouse']}: {row['Qty']:,}ë°•ìŠ¤, {row['SQM']:,.0f}SQM")
    
    # í˜„ì¥ë³„ ì„±ê³¼
    if 'site_performance' in kpi_dashboard and not kpi_dashboard['site_performance'].empty:
        print(f"\nğŸ—ï¸ í˜„ì¥ë³„ ë°°ì†¡ í˜„í™©:")
        for _, row in kpi_dashboard['site_performance'].head().iterrows():
            print(f"     {row['Site']}: {row['Qty']:,}ë°•ìŠ¤, {row['SQM']:,.0f}SQM")
    
    # ì£¼ìš” íë¦„
    if integrated_flow and 'flow_summary' in integrated_flow and not integrated_flow['flow_summary'].empty:
        print(f"\nğŸ”„ ì£¼ìš” ì°½ê³ â†’í˜„ì¥ íë¦„:")
        top_flows = integrated_flow['flow_summary'].nlargest(5, 'Qty')
        for _, row in top_flows.iterrows():
            print(f"     {row['Source_Warehouse']} â†’ {row['Destination_Site']}: {row['Qty']:,}ë°•ìŠ¤")
    
    print(f"\nğŸ¯ HVDC Warehouse Ontology ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“‹ ìƒì„±ëœ ë¦¬í¬íŠ¸: {report_filename}")
    if timeline_results:
        print(f"ğŸ” 18+ ì‹œíŠ¸ì˜ ìƒì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”! (Timeline ë¶„ì„ í¬í•¨)")
    else:
        print(f"ğŸ” 15+ ì‹œíŠ¸ì˜ ìƒì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!")

if __name__ == "__main__":
    main()