# hvdc_ontology_pipeline.py
"""
HVDC Warehouse Analysis Pipeline - Ontology-Enhanced Version
ì´ ë²„ì „ì€ mapping_rules_v2.4.jsonì˜ ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë£°ì„ ë°˜ì˜í•˜ì—¬
TransportEvent, StockSnapshot, DeadStock ë“±ì˜ í´ë˜ìŠ¤ì™€ ì†ì„±ì„ ì •í™•íˆ ë§¤í•‘í•©ë‹ˆë‹¤.

Key Features:
1. ğŸ¯ Refined Transaction Types: FINAL_OUT vs TRANSFER_OUT ì •í™•í•œ ë¶„ë¥˜
2. âœ… Automated Validation: (Opening + Inbound - Outbound = Closing) ìë™ ê²€ì¦
3. ğŸ“Š Dead Stock Analysis: 180ì¼+ ë¯¸ì´ë™ ì¬ê³  ì‹ë³„
4. ğŸ”— Ontology Mapping: RDF/TTL ì¶œë ¥ì„ ìœ„í•œ í‘œì¤€í™”ëœ ë°ì´í„° êµ¬ì¡°
5. ğŸ“ˆ Enhanced Reporting: ì°½ê³ ë³„/ì›”ë³„/ì‚¬ì´íŠ¸ë³„ ìƒì„¸ ë¶„ì„
"""

import pandas as pd
import numpy as np
import os
import glob
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# 1. ONTOLOGY CONFIGURATION
# =============================================================================

class OntologyMapper:
    """ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë£° ê¸°ë°˜ ë°ì´í„° ë³€í™˜ê¸°"""
    
    def __init__(self, mapping_file: str = "mapping_rules_v2.4.json"):
        """ë§¤í•‘ ë£° ë¡œë“œ"""
        try:
            with open(mapping_file, 'r', encoding='utf-8') as f:
                self.mapping_rules = json.load(f)
            print(f"âœ… ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë£° ë¡œë“œ ì™„ë£Œ: {mapping_file}")
        except FileNotFoundError:
            print(f"âš ï¸ ë§¤í•‘ ë£° íŒŒì¼ ì—†ìŒ: {mapping_file}, ê¸°ë³¸ ë§¤í•‘ ì‚¬ìš©")
            self.mapping_rules = self._get_default_mapping()
        
        self.namespace = self.mapping_rules.get("namespace", "http://samsung.com/project-logistics#")
        self.class_mappings = self.mapping_rules.get("class_mappings", {})
        self.property_mappings = self.mapping_rules.get("property_mappings", {})
    
    def _get_default_mapping(self) -> Dict:
        """ê¸°ë³¸ ë§¤í•‘ ë£°"""
        return {
            "namespace": "http://samsung.com/project-logistics#",
            "class_mappings": {
                "TransportEvent": "TransportEvent",
                "StockSnapshot": "StockSnapshot", 
                "DeadStock": "DeadStock",
                "Case": "Case",
                "Warehouse": "Warehouse",
                "Site": "Site"
            },
            "property_mappings": {}
        }
    
    def map_dataframe_columns(self, df: pd.DataFrame, target_class: str) -> pd.DataFrame:
        """ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ì„ ì˜¨í†¨ë¡œì§€ ì†ì„±ì— ë§¤í•‘"""
        mapped_df = df.copy()
        
        # ì»¬ëŸ¼ëª…ì„ ì˜¨í†¨ë¡œì§€ ì†ì„±ìœ¼ë¡œ ë³€í™˜
        column_mapping = {}
        for col in df.columns:
            if col in self.property_mappings:
                prop_info = self.property_mappings[col]
                if prop_info.get("subject_class") == target_class:
                    column_mapping[col] = prop_info["predicate"]
        
        if column_mapping:
            mapped_df = mapped_df.rename(columns=column_mapping)
            print(f"ğŸ”— {target_class} í´ë˜ìŠ¤: {len(column_mapping)}ê°œ ì†ì„± ë§¤í•‘ ì™„ë£Œ")
        
        return mapped_df
    
    def export_to_ttl(self, data_dict: Dict[str, pd.DataFrame], output_file: str):
        """RDF/TTL í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¶œë ¥"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„ ì–¸
                f.write(f"@prefix ex: <{self.namespace}> .\n")
                f.write("@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n\n")
                
                # ê° í´ë˜ìŠ¤ë³„ ë°ì´í„° ì¶œë ¥
                for class_name, df in data_dict.items():
                    if df.empty:
                        continue
                    
                    ontology_class = self.class_mappings.get(class_name, class_name)
                    f.write(f"# {ontology_class} instances\n")
                    
                    for idx, row in df.iterrows():
                        subject_uri = f"ex:{class_name}_{idx}"
                        f.write(f"{subject_uri} a ex:{ontology_class} ;\n")
                        
                        # ì†ì„± ì¶œë ¥
                        properties = []
                        for col, value in row.items():
                            if pd.notna(value):
                                prop_name = f"ex:{col}"
                                if isinstance(value, (int, float)):
                                    properties.append(f"    {prop_name} {value}")
                                elif isinstance(value, datetime):
                                    properties.append(f'    {prop_name} "{value.isoformat()}"^^xsd:dateTime')
                                else:
                                    properties.append(f'    {prop_name} "{str(value)}"')
                        
                        f.write(" ;\n".join(properties))
                        f.write(" .\n\n")
            
            print(f"ğŸ“„ RDF/TTL ì¶œë ¥ ì™„ë£Œ: {output_file}")
            
        except Exception as e:
            print(f"âŒ TTL ì¶œë ¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# 2. ENHANCED DATA UTILITIES
# =============================================================================

def find_column(df: pd.DataFrame, patterns: List[str]) -> Optional[str]:
    """ì»¬ëŸ¼ íŒ¨í„´ ë§¤ì¹­"""
    df_cols_lower = {str(col).lower(): str(col) for col in df.columns}
    for pattern in patterns:
        p_lower = pattern.lower()
        for col_lower, col_original in df_cols_lower.items():
            if p_lower in col_lower:
                return col_original
    return None

def normalize_warehouse_name(raw_name: Any) -> str:
    """ì°½ê³ ëª… í‘œì¤€í™” - ì˜¨í†¨ë¡œì§€ Warehouse í´ë˜ìŠ¤ ëŒ€ì‘"""
    if pd.isna(raw_name):
        return 'UNKNOWN'
    name_lower = str(raw_name).lower().strip()
    
    # ë§¤í•‘ ë£° ê¸°ë°˜ ì •ê·œí™”
    warehouse_rules = {
        'DSV Indoor': ['indoor', 'm44', 'hauler.*indoor'],
        'DSV Al Markaz': ['markaz', 'm1', 'al.*markaz'],
        'DSV Outdoor': ['outdoor', 'out'],
        'MOSB': ['mosb'],
        'DSV MZP': ['mzp'],
        'DHL WH': ['dhl'],
        'AAA Storage': ['aaa']
    }
    
    for canonical, patterns in warehouse_rules.items():
        if any(p in name_lower for p in patterns):
            return canonical
    return str(raw_name).strip()

def normalize_site_name(raw_name: Any) -> str:
    """ì‚¬ì´íŠ¸ëª… í‘œì¤€í™” - ì˜¨í†¨ë¡œì§€ Site í´ë˜ìŠ¤ ëŒ€ì‘"""
    if pd.isna(raw_name):
        return 'UNK'
    name_upper = str(raw_name).upper()
    
    site_patterns = {
        'AGI': ['AGI'],
        'DAS': ['DAS'], 
        'MIR': ['MIR'],
        'SHU': ['SHU']
    }
    
    for site, patterns in site_patterns.items():
        if any(p in name_upper for p in patterns):
            return site
    return 'UNK'

# =============================================================================
# 3. ENHANCED DATA LOADER
# =============================================================================

class EnhancedDataLoader:
    """í–¥ìƒëœ ë°ì´í„° ë¡œë” - ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì§€ì›"""
    
    def __init__(self, ontology_mapper: OntologyMapper):
        self.mapper = ontology_mapper
    
    def load_and_process_files(self, data_dir: str = ".") -> pd.DataFrame:
        """ëª¨ë“  ê´€ë ¨ Excel íŒŒì¼ì„ ë¡œë“œí•˜ê³  í‘œì¤€í™”ëœ TransportEventë¡œ ë³€í™˜"""
        all_movements = []
        
        # HVDC ì°½ê³  íŒŒì¼ íŒ¨í„´
        file_patterns = [
            "HVDC WAREHOUSE_HITACHI*.xlsx",
            "HVDC WAREHOUSE_SIMENSE*.xlsx"
        ]
        
        for pattern in file_patterns:
            for filepath in glob.glob(os.path.join(data_dir, pattern)):
                filename = os.path.basename(filepath)
                print(f"ğŸ“„ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {filename}")
                
                try:
                    # ì¸ë³´ì´ìŠ¤ íŒŒì¼ ìŠ¤í‚µ
                    if 'invoice' in filename.lower():
                        print(f"   - ì¸ë³´ì´ìŠ¤ íŒŒì¼ ìŠ¤í‚µ")
                        continue
                    
                    movements = self._process_warehouse_file(filepath)
                    if not movements.empty:
                        all_movements.append(movements)
                        print(f"   âœ… {len(movements)}ê±´ ì´ë²¤íŠ¸ ì¶”ì¶œ")
                    
                except Exception as e:
                    print(f"   âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        if not all_movements:
            print("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return pd.DataFrame()
        
        # ëª¨ë“  ì´ë™ ê¸°ë¡ í†µí•©
        combined_df = pd.concat(all_movements, ignore_index=True)
        print(f"ğŸ“Š ì´ {len(combined_df):,}ê±´ì˜ ì›ì‹œ ì´ë²¤íŠ¸ ìˆ˜ì§‘")
        
        return combined_df
    
    def _process_warehouse_file(self, filepath: str) -> pd.DataFrame:
        """ê°œë³„ ì°½ê³  íŒŒì¼ ì²˜ë¦¬"""
        try:
            # Excel íŒŒì¼ ë¡œë“œ
            xl_file = pd.ExcelFile(filepath)
            sheet_name = xl_file.sheet_names[0]
            
            # Case List ì‹œíŠ¸ ìš°ì„  ì„ íƒ
            for sheet in xl_file.sheet_names:
                if 'case' in sheet.lower() and 'list' in sheet.lower():
                    sheet_name = sheet
                    break
            
            df = pd.read_excel(filepath, sheet_name=sheet_name)
            
            # í•µì‹¬ ì»¬ëŸ¼ ì°¾ê¸°
            case_col = find_column(df, ['case', 'carton', 'box', 'mr#', 'sct ship no', 'case no'])
            qty_col = find_column(df, ["q'ty", 'qty', 'quantity', 'received', "p'kg", 'pkg'])
            
            if not case_col:
                print(f"   âš ï¸ Case ì»¬ëŸ¼ ì—†ìŒ")
                return pd.DataFrame()
            
            # ë‚ ì§œ ì»¬ëŸ¼ ì‹ë³„ (ì°½ê³ /ì‚¬ì´íŠ¸ ì´ë™ ê¸°ë¡)
            date_cols = []
            for col in df.columns:
                # ë‚ ì§œ íŒ¨í„´ì´ ìˆëŠ” ì»¬ëŸ¼ ì°¾ê¸°
                sample_values = df[col].dropna().astype(str).head(10)
                if any(self._is_date_like(val) for val in sample_values):
                    date_cols.append(col)
            
            print(f"   ğŸ“… ë‚ ì§œ ì»¬ëŸ¼ {len(date_cols)}ê°œ ë°œê²¬")
            
            # ì´ë²¤íŠ¸ ì¶”ì¶œ
            movements = []
            for idx, row in df.iterrows():
                case_no = str(row[case_col]) if pd.notna(row[case_col]) else f"CASE_{idx}"
                qty = pd.to_numeric(row.get(qty_col, 1), errors='coerce') or 1
                
                # ê° ë‚ ì§œ ì»¬ëŸ¼ì—ì„œ ì´ë²¤íŠ¸ ì¶”ì¶œ
                case_events = []
                for date_col in date_cols:
                    if pd.notna(row[date_col]):
                        event_date = pd.to_datetime(row[date_col], errors='coerce')
                        if pd.notna(event_date):
                            # ğŸ¯ ìˆ˜ì •: ë‚ ì§œ ì»¬ëŸ¼ëª…ì´ ì•„ë‹Œ ì‹¤ì œ ì°½ê³ ëª… ì¶”ì¶œ
                            location = self._extract_warehouse_from_column_name(date_col)
                            if location != 'UNKNOWN':  # ìœ íš¨í•œ ì°½ê³ ëª…ë§Œ ì²˜ë¦¬
                                case_events.append({
                                    'Case_No': case_no,
                                    'Date': event_date,
                                    'Qty': qty,
                                    'Location': location,
                                    'Raw_Location': str(date_col),
                                    'Source_File': os.path.basename(filepath)
                                })
                
                # ì‹œê°„ìˆœ ì •ë ¬
                case_events.sort(key=lambda x: x['Date'])
                movements.extend(case_events)
            
            return pd.DataFrame(movements)
            
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
    
    def _is_date_like(self, value: str) -> bool:
        """ë¬¸ìì—´ì´ ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸"""
        date_patterns = [
            r'\d{4}-\d{2}-\d{2}',  # 2024-01-01
            r'\d{2}/\d{2}/\d{4}',  # 01/01/2024
            r'\d{1,2}/\d{1,2}/\d{4}',  # 1/1/2024
        ]
        import re
        return any(re.search(pattern, str(value)) for pattern in date_patterns)
    
    def _extract_warehouse_from_column_name(self, col_name: str) -> str:
        """ì»¬ëŸ¼ëª…ì—ì„œ ì‹¤ì œ ì°½ê³ ëª… ì¶”ì¶œ (ë‚ ì§œ í•„ë“œ ì œì™¸)"""
        col_lower = str(col_name).lower()
        
        # ğŸš« ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ë“¤ì€ ì°½ê³ ê°€ ì•„ë‹˜
        date_keywords = ['etd', 'eta', 'atd', 'ata', 'date', 'time', 'departure', 'arrival']
        if any(keyword in col_lower for keyword in date_keywords):
            return 'UNKNOWN'
        
        # ğŸ¢ ì‹¤ì œ ì°½ê³ ëª… íŒ¨í„´ ë§¤ì¹­
        warehouse_patterns = {
            'DSV Indoor': ['indoor', 'm44', 'hauler indoor'],
            'DSV Outdoor': ['outdoor', 'out '],
            'DSV Al Markaz': ['markaz', 'al markaz', 'm1'],
            'MOSB': ['mosb', 'barge'],
            'DSV MZP': ['mzp'],
            'DHL WH': ['dhl'],
            'AAA Storage': ['aaa'],
            'Shifting': ['shifting']
        }
        
        for warehouse, patterns in warehouse_patterns.items():
            if any(pattern in col_lower for pattern in patterns):
                return warehouse
        
        # ì‚¬ì´íŠ¸ íŒ¨í„´ë„ í™•ì¸
        site_patterns = ['agi', 'das', 'mir', 'shu']
        for site in site_patterns:
            if site in col_lower:
                return site.upper()
        
        return 'UNKNOWN'

# =============================================================================
# 4. ENHANCED TRANSACTION ENGINE
# =============================================================================

class EnhancedTransactionEngine:
    """í–¥ìƒëœ íŠ¸ëœì­ì…˜ ì—”ì§„ - ì˜¨í†¨ë¡œì§€ TransportEvent ë§¤í•‘"""
    
    def __init__(self, ontology_mapper: OntologyMapper):
        self.mapper = ontology_mapper
    
    def create_transaction_log(self, raw_events: pd.DataFrame) -> pd.DataFrame:
        """ì›ì‹œ ì´ë²¤íŠ¸ë¥¼ í‘œì¤€í™”ëœ TransportEvent ë¡œê·¸ë¡œ ë³€í™˜"""
        if raw_events.empty:
            return pd.DataFrame()
        
        print("ğŸ”„ íŠ¸ëœì­ì…˜ ë¡œê·¸ ìƒì„± ì¤‘...")
        
        # ë‚ ì§œìˆœ ì •ë ¬
        raw_events = raw_events.sort_values(['Case_No', 'Date']).reset_index(drop=True)
        
        transactions = []
        
        # ì¼€ì´ìŠ¤ë³„ë¡œ ì´ë²¤íŠ¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬
        for case_no, group in raw_events.groupby('Case_No'):
            group = group.reset_index(drop=True)
            
            for i, row in group.iterrows():
                # ì´ì „ ìœ„ì¹˜ (FROM)
                loc_from = group.loc[i-1, 'Location'] if i > 0 else 'SOURCE'
                loc_to = row['Location']
                
                # ğŸ¯ í•µì‹¬: TxType_Refined ë¶„ë¥˜
                # 1. IN íŠ¸ëœì­ì…˜ (í˜„ì¬ ìœ„ì¹˜ë¡œ ì…ê³ )
                tx_in = {
                    'Tx_ID': f"{case_no}_{row['Date'].strftime('%Y%m%d%H%M%S')}_IN",
                    'Case_No': case_no,
                    'Date': row['Date'],
                    'Qty': row['Qty'],
                    'TxType': 'IN',
                    'TxType_Refined': 'IN',
                    'Loc_From': loc_from,
                    'Loc_To': loc_to,
                    'Location': loc_to,  # ì¬ê³  ê³„ì‚°ìš©
                    'Site': normalize_site_name(loc_to),
                    'Source_File': row['Source_File']
                }
                transactions.append(tx_in)
                
                # 2. OUT íŠ¸ëœì­ì…˜ (ì´ì „ ìœ„ì¹˜ì—ì„œ ì¶œê³ )
                if i > 0:  # ì²« ë²ˆì§¸ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ OUT ìƒì„±
                    prev_location = group.loc[i-1, 'Location']
                    
                    # ğŸ¯ FINAL_OUT vs TRANSFER_OUT ë¶„ë¥˜
                    site_name = normalize_site_name(loc_to)
                    if site_name != 'UNK':
                        # í˜„ì¥ìœ¼ë¡œ ë°°ì†¡ = FINAL_OUT
                        tx_type_refined = 'FINAL_OUT'
                    else:
                        # ì°½ê³ ê°„ ì´ë™ = TRANSFER_OUT
                        tx_type_refined = 'TRANSFER_OUT'
                    
                    tx_out = {
                        'Tx_ID': f"{case_no}_{row['Date'].strftime('%Y%m%d%H%M%S')}_OUT",
                        'Case_No': case_no,
                        'Date': row['Date'],
                        'Qty': row['Qty'],
                        'TxType': 'OUT',
                        'TxType_Refined': tx_type_refined,
                        'Loc_From': prev_location,
                        'Loc_To': loc_to,
                        'Location': prev_location,  # ì¬ê³  ê³„ì‚°ìš© (ì¶œê³  ìœ„ì¹˜)
                        'Site': site_name,
                        'Source_File': row['Source_File']
                    }
                    transactions.append(tx_out)
        
        tx_df = pd.DataFrame(transactions)
        
        if not tx_df.empty:
            # ì¤‘ë³µ ì œê±°
            tx_df = tx_df.drop_duplicates(subset=['Tx_ID']).reset_index(drop=True)
            
            # íŠ¸ëœì­ì…˜ íƒ€ì… ë¶„í¬ ì¶œë ¥
            print("ğŸ“Š íŠ¸ëœì­ì…˜ íƒ€ì… ë¶„í¬:")
            type_counts = tx_df['TxType_Refined'].value_counts()
            for tx_type, count in type_counts.items():
                percentage = (count / len(tx_df)) * 100
                print(f"   {tx_type}: {count:,}ê±´ ({percentage:.1f}%)")
        
        return tx_df

# =============================================================================
# 5. ENHANCED ANALYSIS ENGINE
# =============================================================================

class EnhancedAnalysisEngine:
    """í–¥ìƒëœ ë¶„ì„ ì—”ì§„ - StockSnapshot ë° DeadStock ìƒì„±"""
    
    def __init__(self, ontology_mapper: OntologyMapper):
        self.mapper = ontology_mapper
    
    def calculate_daily_stock(self, tx_df: pd.DataFrame) -> pd.DataFrame:
        """ì¼ë³„ ì¬ê³  ê³„ì‚° - StockSnapshot í´ë˜ìŠ¤ ë§¤í•‘"""
        if tx_df.empty:
            return pd.DataFrame()
        
        print("ğŸ“Š ì¼ë³„ ì¬ê³  ê³„ì‚° ì¤‘...")
        
        # ë‚ ì§œë³„, ìœ„ì¹˜ë³„ ì§‘ê³„
        tx_df['Date'] = pd.to_datetime(tx_df['Date']).dt.date
        
        daily_summary = tx_df.groupby(['Location', 'Date', 'TxType_Refined']).agg({
            'Qty': 'sum'
        }).reset_index()
        
        # í”¼ë²—ìœ¼ë¡œ ì…ê³ /ì¶œê³  ë¶„ë¦¬
        daily_pivot = daily_summary.pivot_table(
            index=['Location', 'Date'],
            columns='TxType_Refined', 
            values='Qty',
            fill_value=0
        ).reset_index()
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        daily_pivot.columns.name = None
        expected_cols = ['IN', 'TRANSFER_OUT', 'FINAL_OUT']
        for col in expected_cols:
            if col not in daily_pivot.columns:
                daily_pivot[col] = 0
        
        # ì¬ê³  ê³„ì‚° (ìœ„ì¹˜ë³„ ëˆ„ì )
        stock_records = []
        
        for location in daily_pivot['Location'].unique():
            if location in ['UNKNOWN', 'UNK']:
                continue
                
            loc_data = daily_pivot[daily_pivot['Location'] == location].copy()
            loc_data = loc_data.sort_values('Date')
            
            opening_stock = 0
            
            for _, row in loc_data.iterrows():
                inbound = row['IN']
                transfer_out = row['TRANSFER_OUT'] 
                final_out = row['FINAL_OUT']
                total_outbound = transfer_out + final_out
                
                closing_stock = opening_stock + inbound - total_outbound
                
                stock_records.append({
                    'Location': location,
                    'Date': row['Date'],
                    'Opening_Stock': opening_stock,
                    'Inbound': inbound,
                    'Transfer_Out': transfer_out,
                    'Final_Out': final_out,
                    'Total_Outbound': total_outbound,
                    'Closing_Stock': closing_stock,
                    'Date_Snapshot': row['Date']  # ì˜¨í†¨ë¡œì§€ ë§¤í•‘ìš©
                })
                
                opening_stock = closing_stock
        
        daily_stock_df = pd.DataFrame(stock_records)
        print(f"âœ… {len(daily_stock_df)}ê°œ ì¼ë³„ ì¬ê³  ìŠ¤ëƒ…ìƒ· ìƒì„±")
        
        return daily_stock_df
    
    def validate_stock_integrity(self, daily_stock_df: pd.DataFrame) -> Dict[str, Any]:
        """ì¬ê³  ë¬´ê²°ì„± ê²€ì¦ - (Opening + Inbound - Outbound = Closing)"""
        print("ğŸ”¬ ì¬ê³  ë¬´ê²°ì„± ê²€ì¦ ì¤‘...")
        
        if daily_stock_df.empty:
            return {"status": "SKIP", "message": "ê²€ì¦í•  ë°ì´í„° ì—†ìŒ"}
        
        validation_results = []
        total_errors = 0
        
        for _, row in daily_stock_df.iterrows():
            expected_closing = row['Opening_Stock'] + row['Inbound'] - row['Total_Outbound']
            actual_closing = row['Closing_Stock']
            difference = abs(actual_closing - expected_closing)
            
            if difference > 0.01:  # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ í—ˆìš©
                total_errors += 1
                validation_results.append({
                    'Location': row['Location'],
                    'Date': row['Date'],
                    'Expected': expected_closing,
                    'Actual': actual_closing,
                    'Difference': difference
                })
        
        if total_errors == 0:
            print("âœ… ê²€ì¦ í†µê³¼! ëª¨ë“  ì¬ê³  ê³„ì‚°ì´ ì •í™•í•©ë‹ˆë‹¤.")
            return {"status": "PASS", "errors": 0, "details": []}
        else:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨! {total_errors}ê°œ ì˜¤ë¥˜ ë°œê²¬")
            return {"status": "FAIL", "errors": total_errors, "details": validation_results[:10]}
    
    def analyze_dead_stock(self, tx_df: pd.DataFrame, threshold_days: int = 180) -> pd.DataFrame:
        """ì¥ê¸° ì²´í™” ì¬ê³  ë¶„ì„ - DeadStock í´ë˜ìŠ¤ ë§¤í•‘"""
        print(f"ğŸ“¦ ì¥ê¸° ì²´í™” ì¬ê³  ë¶„ì„ (ê¸°ì¤€: {threshold_days}ì¼)...")
        
        if tx_df.empty:
            return pd.DataFrame()
        
        # ê° ì¼€ì´ìŠ¤ì˜ ë§ˆì§€ë§‰ ì´ë™ ë‚ ì§œ ê³„ì‚°
        tx_df['Date'] = pd.to_datetime(tx_df['Date'])
        latest_moves = tx_df.groupby('Case_No')['Date'].max().reset_index()
        latest_moves.columns = ['Case_No', 'Last_Move_Date']
        
        # í˜„ì¬ ë‚ ì§œì™€ ë¹„êµ
        current_date = datetime.now()
        latest_moves['Days_Since_Last_Move'] = (current_date - latest_moves['Last_Move_Date']).dt.days
        
        # ì¥ê¸° ì²´í™” ì¬ê³  í•„í„°ë§
        dead_stock = latest_moves[latest_moves['Days_Since_Last_Move'] >= threshold_days].copy()
        
        if not dead_stock.empty:
            # ì¶”ê°€ ì •ë³´ ì¡°ì¸
            case_info = tx_df.groupby('Case_No').agg({
                'Qty': 'first',
                'Location': 'last',  # ë§ˆì§€ë§‰ ìœ„ì¹˜
                'Source_File': 'first'
            }).reset_index()
            
            dead_stock = dead_stock.merge(case_info, on='Case_No', how='left')
            
            print(f"âš ï¸ {len(dead_stock)}ê°œ ì¥ê¸° ì²´í™” ì¬ê³  ì¼€ì´ìŠ¤ ë°œê²¬")
        else:
            print("âœ… ì¥ê¸° ì²´í™” ì¬ê³  ì—†ìŒ")
        
        return dead_stock
    
    def create_monthly_summary(self, tx_df: pd.DataFrame, daily_stock: pd.DataFrame) -> pd.DataFrame:
        """ì›”ë³„ ìš”ì•½ ìƒì„±"""
        print("ğŸ“… ì›”ë³„ ìš”ì•½ ìƒì„± ì¤‘...")
        
        if tx_df.empty:
            return pd.DataFrame()
        
        # ì›”ë³„ íŠ¸ëœì­ì…˜ ì§‘ê³„
        tx_df['YearMonth'] = pd.to_datetime(tx_df['Date']).dt.to_period('M').astype(str)
        
        monthly_tx = tx_df.groupby(['Location', 'YearMonth', 'TxType_Refined']).agg({
            'Qty': 'sum'
        }).reset_index()
        
        # í”¼ë²—
        monthly_pivot = monthly_tx.pivot_table(
            index=['Location', 'YearMonth'],
            columns='TxType_Refined',
            values='Qty',
            fill_value=0
        ).reset_index()
        
        monthly_pivot.columns.name = None
        
        # í•„ìš”í•œ ì»¬ëŸ¼ ì¶”ê°€
        for col in ['IN', 'TRANSFER_OUT', 'FINAL_OUT']:
            if col not in monthly_pivot.columns:
                monthly_pivot[col] = 0
        
        # ì›”ë§ ì¬ê³  ì¶”ê°€
        if not daily_stock.empty:
            # ê° ìœ„ì¹˜ë³„ ì›”ë³„ ë§ˆì§€ë§‰ ì¬ê³ 
            daily_stock['YearMonth'] = pd.to_datetime(daily_stock['Date']).dt.to_period('M').astype(str)
            monthly_closing = daily_stock.groupby(['Location', 'YearMonth'])['Closing_Stock'].last().reset_index()
            
            monthly_pivot = monthly_pivot.merge(monthly_closing, on=['Location', 'YearMonth'], how='left')
            monthly_pivot['Closing_Stock'] = monthly_pivot['Closing_Stock'].fillna(0)
        else:
            monthly_pivot['Closing_Stock'] = 0
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        column_order = ['Location', 'YearMonth', 'IN', 'TRANSFER_OUT', 'FINAL_OUT', 'Closing_Stock']
        monthly_pivot = monthly_pivot[column_order]
        
        print(f"âœ… {len(monthly_pivot)}ê°œ ì›”ë³„ ìš”ì•½ ë ˆì½”ë“œ ìƒì„±")
        
        return monthly_pivot

# =============================================================================
# 6. ENHANCED REPORT WRITER
# =============================================================================

class EnhancedReportWriter:
    """í–¥ìƒëœ ë¦¬í¬íŠ¸ ì‘ì„±ê¸° - ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì§€ì›"""
    
    def __init__(self, ontology_mapper: OntologyMapper):
        self.mapper = ontology_mapper
    
    def save_comprehensive_report(self, analysis_results: Dict[str, pd.DataFrame], output_path: str):
        """ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥ (Excel + RDF/TTL)"""
        print(f"ğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±: {output_path}")
        
        # Excel ë¦¬í¬íŠ¸ ì €ì¥
        self._save_excel_report(analysis_results, output_path)
        
        # RDF/TTL ë¦¬í¬íŠ¸ ì €ì¥
        ttl_path = output_path.replace('.xlsx', '.ttl')
        self._save_rdf_report(analysis_results, ttl_path)
    
    def _save_excel_report(self, data: Dict[str, pd.DataFrame], output_path: str):
        """Excel ë¦¬í¬íŠ¸ ì €ì¥"""
        try:
            with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
                workbook = writer.book
                
                # ì„œì‹ ì •ì˜
                header_format = workbook.add_format({
                    'bold': True,
                    'text_wrap': True,
                    'valign': 'top',
                    'fg_color': '#D7E4BC',
                    'border': 1,
                    'align': 'center'
                })
                
                number_format = workbook.add_format({'num_format': '#,##0'})
                date_format = workbook.add_format({'num_format': 'yyyy-mm-dd'})
                
                # ì‹œíŠ¸ë³„ ì €ì¥
                sheet_configs = {
                    'transaction_log': 'ì „ì²´_íŠ¸ëœì­ì…˜_ë¡œê·¸',
                    'daily_stock': 'ì¼ë³„_ì¬ê³ _ìƒì„¸',
                    'monthly_summary': 'ì›”ë³„_ì…ì¶œê³ _ì¬ê³ _ìš”ì•½',
                    'dead_stock': 'ì¥ê¸°ì²´í™”ì¬ê³ _ë¶„ì„',
                    'validation_results': 'ë¬´ê²°ì„±_ê²€ì¦_ê²°ê³¼'
                }
                
                for data_key, sheet_name in sheet_configs.items():
                    if data_key in data and not data[data_key].empty:
                        df = data[data_key]
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                        
                        # ì„œì‹ ì ìš©
                        worksheet = writer.sheets[sheet_name]
                        
                        # í—¤ë” ì„œì‹
                        for col_num, value in enumerate(df.columns.values):
                            worksheet.write(0, col_num, value, header_format)
                        
                        # ì»¬ëŸ¼ë³„ ì„œì‹
                        for i, col in enumerate(df.columns):
                            if 'Qty' in col or 'Stock' in col or col in ['IN', 'TRANSFER_OUT', 'FINAL_OUT']:
                                worksheet.set_column(i, i, 12, number_format)
                            elif 'Date' in col:
                                worksheet.set_column(i, i, 12, date_format)
                            else:
                                worksheet.set_column(i, i, 15)
            
            print(f"âœ… Excel ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {output_path}")
            
        except Exception as e:
            print(f"âŒ Excel ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_rdf_report(self, data: Dict[str, pd.DataFrame], ttl_path: str):
        """RDF/TTL ë¦¬í¬íŠ¸ ì €ì¥"""
        try:
            # ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ë³„ ë°ì´í„° ë§¤í•‘
            ontology_data = {}
            
            # TransportEvent
            if 'transaction_log' in data:
                tx_df = data['transaction_log'].copy()
                ontology_data['TransportEvent'] = self.mapper.map_dataframe_columns(tx_df, 'TransportEvent')
            
            # StockSnapshot  
            if 'daily_stock' in data:
                stock_df = data['daily_stock'].copy()
                ontology_data['StockSnapshot'] = self.mapper.map_dataframe_columns(stock_df, 'StockSnapshot')
            
            # DeadStock
            if 'dead_stock' in data:
                dead_df = data['dead_stock'].copy()
                ontology_data['DeadStock'] = self.mapper.map_dataframe_columns(dead_df, 'DeadStock')
            
            # TTL ì¶œë ¥
            self.mapper.export_to_ttl(ontology_data, ttl_path)
            
        except Exception as e:
            print(f"âŒ RDF/TTL ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# 7. MAIN PIPELINE
# =============================================================================

def main():
    """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("ğŸš€ HVDC ì˜¨í†¨ë¡œì§€ ê°•í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    try:
        # 1. ì˜¨í†¨ë¡œì§€ ë§¤í¼ ì´ˆê¸°í™”
        mapper = OntologyMapper("mapping_rules_v2.4.json")
        
        print(f"âœ… ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë£° ë°˜ì˜ ì™„ë£Œ!")
        print(f"   ğŸ”— ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {mapper.namespace}")
        print(f"   ğŸ“Š í´ë˜ìŠ¤ ë§¤í•‘: {len(mapper.class_mappings)}ê°œ")
        print(f"   ğŸ·ï¸ ì†ì„± ë§¤í•‘: {len(mapper.property_mappings)}ê°œ")
        print()
        
        # 2. ë°ì´í„° ë¡œë” ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë”©
        loader = EnhancedDataLoader(mapper)
        print("ğŸ“„ ë°ì´í„° íŒŒì¼ ë¡œë”© ì¤‘...")
        raw_data = loader.load_and_process_files("data")
        
        if raw_data.empty:
            print("âš ï¸ ë¡œë”©ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. data/ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            return False
        
        print(f"âœ… ì´ {len(raw_data)}ê°œ ì´ë²¤íŠ¸ ë¡œë”© ì™„ë£Œ")
        print()
        
        # 3. íŠ¸ëœì­ì…˜ ì—”ì§„ìœ¼ë¡œ ë¡œê·¸ ìƒì„±
        tx_engine = EnhancedTransactionEngine(mapper)
        print("ğŸ”„ íŠ¸ëœì­ì…˜ ë¡œê·¸ ìƒì„± ì¤‘...")
        transaction_log = tx_engine.create_transaction_log(raw_data)
        
        # 4. ë¶„ì„ ì—”ì§„ìœ¼ë¡œ ìƒì„¸ ë¶„ì„
        analyzer = EnhancedAnalysisEngine(mapper)
        
        print("ğŸ“Š ì¼ë³„ ì¬ê³  ê³„ì‚° ì¤‘...")
        daily_stock = analyzer.calculate_daily_stock(transaction_log)
        
        print("ğŸ” ì¬ê³  ë¬´ê²°ì„± ê²€ì¦ ì¤‘...")
        validation_results = analyzer.validate_stock_integrity(daily_stock)
        
        print("âš ï¸ ì¥ê¸° ì²´í™” ì¬ê³  ë¶„ì„ ì¤‘...")
        dead_stock = analyzer.analyze_dead_stock(transaction_log)
        
        print("ğŸ“… ì›”ë³„ ìš”ì•½ ìƒì„± ì¤‘...")
        monthly_summary = analyzer.create_monthly_summary(transaction_log, daily_stock)
        
        # 5. ê²°ê³¼ ì¢…í•©
        analysis_results = {
            'transaction_log': transaction_log,
            'daily_stock': daily_stock,
            'monthly_summary': monthly_summary,
            'dead_stock': dead_stock,
            'validation_results': pd.DataFrame([validation_results])
        }
        
        # 6. ë¦¬í¬íŠ¸ ì €ì¥
        report_writer = EnhancedReportWriter(mapper)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"HVDC_í†µí•©_ì˜¨í†¨ë¡œì§€_ë¶„ì„_ë¦¬í¬íŠ¸_{timestamp}.xlsx"
        
        print("ğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥ ì¤‘...")
        report_writer.save_comprehensive_report(analysis_results, output_path)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ HVDC ì˜¨í†¨ë¡œì§€ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path}")
        print(f"ğŸ”— RDF/TTL íŒŒì¼: {output_path.replace('.xlsx', '.ttl')}")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print(f"\nğŸ‰ ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë£° ë°˜ì˜ ì™„ë£Œ!")
    else:
        print(f"\nğŸ’¥ ë§¤í•‘ ë£° ë°˜ì˜ ì‹¤íŒ¨") 