#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HVDC í†µí•© ì˜¨í†¨ë¡œì§€ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ì¸ë³´ì´ìŠ¤ í¬í•¨)
=============================================

hvdc_ontology_pipeline.pyë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¸ë³´ì´ìŠ¤ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬
ì™„ì „í•œ ë¬¼ë¥˜-ì¬ë¬´ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ğŸ¯ ê¸°ì¡´ ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë£° í™œìš© (mapping_rules_v2.4.json)
2. ğŸ’° ì¸ë³´ì´ìŠ¤ ë°ì´í„° í†µí•© ë¶„ì„
3. ğŸ”— ì°½ê³ -ì¸ë³´ì´ìŠ¤ ë°ì´í„° ì—°ê´€ ë¶„ì„
4. ğŸ“Š ì¢…í•© ì¬ë¬´-ìš´ì˜ ë¦¬í¬íŠ¸ ìƒì„±
5. ğŸ§  ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ë°ì´í„° êµ¬ì¡°í™”

ì‘ì„±ì: HVDC Analysis Team
ìµœì¢… ìˆ˜ì •: 2025-06-22
ê¸°ë°˜: hvdc_ontology_pipeline.py
"""

import pandas as pd
import numpy as np
import os
import glob
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# 1. ENHANCED ONTOLOGY CONFIGURATION (ì¸ë³´ì´ìŠ¤ í´ë˜ìŠ¤ ì¶”ê°€)
# =============================================================================

class EnhancedOntologyMapper:
    """ì¸ë³´ì´ìŠ¤ í´ë˜ìŠ¤ë¥¼ í¬í•¨í•œ í–¥ìƒëœ ì˜¨í†¨ë¡œì§€ ë§¤í¼"""
    
    def __init__(self, mapping_file: str = "mapping_rules_v2.4.json"):
        """ë§¤í•‘ ë£° ë¡œë“œ ë° ì¸ë³´ì´ìŠ¤ í´ë˜ìŠ¤ í™•ì¥"""
        try:
            with open(mapping_file, 'r', encoding='utf-8') as f:
                self.mapping_rules = json.load(f)
            print(f"âœ… ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ë£° ë¡œë“œ ì™„ë£Œ: {mapping_file}")
        except FileNotFoundError:
            print(f"âš ï¸ ë§¤í•‘ ë£° íŒŒì¼ ì—†ìŒ: {mapping_file}, ê¸°ë³¸ ë§¤í•‘ ì‚¬ìš©")
            self.mapping_rules = self._get_default_mapping()
        
        self.namespace = self.mapping_rules.get("namespace", "http://samsung.com/project-logistics#")
        self.class_mappings = self.mapping_rules.get("class_mappings", {})
        self.property_mappings = self.mapping_rules.get("property_mappings", {})
        
        # ì¸ë³´ì´ìŠ¤ ê´€ë ¨ í´ë˜ìŠ¤ ì¶”ê°€
        self._extend_for_invoice()
    
    def _extend_for_invoice(self):
        """ì¸ë³´ì´ìŠ¤ ê´€ë ¨ ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ í™•ì¥"""
        invoice_classes = {
            "InvoiceRecord": "InvoiceRecord",
            "ShipmentOperation": "ShipmentOperation", 
            "CostStructure": "CostStructure",
            "FinancialMetrics": "FinancialMetrics",
            "OperationalEfficiency": "OperationalEfficiency"
        }
        
        self.class_mappings.update(invoice_classes)
        
        # ì¸ë³´ì´ìŠ¤ ì†ì„± ë§¤í•‘ ì¶”ê°€
        invoice_properties = {
            "shipment_no": {"predicate": "hasShipmentNumber", "subject_class": "InvoiceRecord"},
            "operation_month": {"predicate": "operationMonth", "subject_class": "InvoiceRecord"},
            "category": {"predicate": "shipmentCategory", "subject_class": "InvoiceRecord"},
            "total_cost": {"predicate": "totalCost", "subject_class": "CostStructure"},
            "handling_in": {"predicate": "handlingInCost", "subject_class": "CostStructure"},
            "handling_out": {"predicate": "handlingOutCost", "subject_class": "CostStructure"},
            "packages_qty": {"predicate": "packageQuantity", "subject_class": "ShipmentOperation"},
            "weight_kg": {"predicate": "weightInKg", "subject_class": "ShipmentOperation"},
            "cbm": {"predicate": "volumeInCBM", "subject_class": "ShipmentOperation"}
        }
        
        self.property_mappings.update(invoice_properties)
        
        print(f"ğŸ”— ì¸ë³´ì´ìŠ¤ ì˜¨í†¨ë¡œì§€ í™•ì¥ ì™„ë£Œ: {len(invoice_classes)}ê°œ í´ë˜ìŠ¤, {len(invoice_properties)}ê°œ ì†ì„±")
    
    def _get_default_mapping(self) -> Dict:
        """ê¸°ë³¸ ë§¤í•‘ ë£° (ì¸ë³´ì´ìŠ¤ í¬í•¨)"""
        return {
            "namespace": "http://samsung.com/project-logistics#",
            "class_mappings": {
                "TransportEvent": "TransportEvent",
                "StockSnapshot": "StockSnapshot", 
                "DeadStock": "DeadStock",
                "Case": "Case",
                "Warehouse": "Warehouse",
                "Site": "Site",
                "InvoiceRecord": "InvoiceRecord",
                "ShipmentOperation": "ShipmentOperation",
                "CostStructure": "CostStructure"
            },
            "property_mappings": {}
        }
    
    def map_dataframe_columns(self, df: pd.DataFrame, target_class: str) -> pd.DataFrame:
        """ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ì„ ì˜¨í†¨ë¡œì§€ ì†ì„±ì— ë§¤í•‘"""
        mapped_df = df.copy()
        
        # ì»¬ëŸ¼ëª…ì„ ì˜¨í†¨ë¡œì§€ ì†ì„±ìœ¼ë¡œ ë³€í™˜
        column_mapping = {}
        for col in df.columns:
            if col in self.property_mappings:
                prop_info = self.property_mappings[col]
                if prop_info.get("subject_class") == target_class:
                    column_mapping[col] = prop_info["predicate"]
        
        if column_mapping:
            mapped_df = mapped_df.rename(columns=column_mapping)
            print(f"ğŸ”— {target_class} í´ë˜ìŠ¤: {len(column_mapping)}ê°œ ì†ì„± ë§¤í•‘ ì™„ë£Œ")
        
        return mapped_df
    
    def export_to_ttl(self, data_dict: Dict[str, pd.DataFrame], output_file: str):
        """RDF/TTL í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¶œë ¥"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„ ì–¸
                f.write(f"@prefix ex: <{self.namespace}> .\n")
                f.write("@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n\n")
                
                # ê° í´ë˜ìŠ¤ë³„ ë°ì´í„° ì¶œë ¥
                for class_name, df in data_dict.items():
                    if df.empty:
                        continue
                    
                    ontology_class = self.class_mappings.get(class_name, class_name)
                    f.write(f"# {ontology_class} instances\n")
                    
                    for idx, row in df.iterrows():
                        subject_uri = f"ex:{class_name}_{idx}"
                        f.write(f"{subject_uri} a ex:{ontology_class} ;\n")
                        
                        # ì†ì„± ì¶œë ¥
                        properties = []
                        for col, value in row.items():
                            if pd.notna(value):
                                prop_name = f"ex:{col}"
                                if isinstance(value, (int, float)):
                                    properties.append(f"    {prop_name} {value}")
                                elif isinstance(value, datetime):
                                    properties.append(f'    {prop_name} "{value.isoformat()}"^^xsd:dateTime')
                                else:
                                    properties.append(f'    {prop_name} "{str(value)}"')
                        
                        f.write(" ;\n".join(properties))
                        f.write(" .\n\n")
            
            print(f"ğŸ“„ RDF/TTL ì¶œë ¥ ì™„ë£Œ: {output_file}")
            
        except Exception as e:
            print(f"âŒ TTL ì¶œë ¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# 2. INVOICE DATA ANALYZER
# =============================================================================

class InvoiceAnalyzer:
    """ì¸ë³´ì´ìŠ¤ ë°ì´í„° ë¶„ì„ê¸° (ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ì§€ì›)"""
    
    def __init__(self, ontology_mapper: EnhancedOntologyMapper):
        self.mapper = ontology_mapper
        self.invoice_df = None
        self.processed_data = {}
        
    def load_invoice_data(self, file_path: str = 'data/HVDC WAREHOUSE_INVOICE.xlsx') -> bool:
        """ì¸ë³´ì´ìŠ¤ ë°ì´í„° ë¡œë“œ"""
        try:
            self.invoice_df = pd.read_excel(file_path)
            print(f"âœ… ì¸ë³´ì´ìŠ¤ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.invoice_df)}ê±´")
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            self._preprocess_invoice_data()
            return True
            
        except Exception as e:
            print(f"âŒ ì¸ë³´ì´ìŠ¤ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _preprocess_invoice_data(self):
        """ì¸ë³´ì´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬"""
        if self.invoice_df is None:
            return
        
        df = self.invoice_df.copy()
        
        # ì»¬ëŸ¼ëª… í‘œì¤€í™”
        column_mapping = {
            'Shipment No': 'shipment_no',
            'Operation Month': 'operation_month',
            'Category': 'category',
            'pkgs q\'ty': 'packages_qty',
            'Weight (kg)': 'weight_kg',
            'CBM': 'cbm',
            'TOTAL': 'total_cost',
            'Handling In': 'handling_in',
            'Handling out': 'handling_out',
            'Start': 'start_date',
            'Finish': 'finish_date'
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df[new_col] = df[old_col]
        
        # HE íŒ¨í„´ ì¶”ì¶œ
        df['extracted_he_pattern'] = df['shipment_no'].str.extract(r'(HE-\d+)', expand=False)
        
        # ë‚ ì§œ í˜•ì‹ í†µì¼
        date_columns = ['operation_month', 'start_date', 'finish_date']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        
        self.invoice_df = df
        print(f"ğŸ”„ ì¸ë³´ì´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê±´")
    
    def analyze_invoice_operations(self) -> Dict[str, Any]:
        """ì¸ë³´ì´ìŠ¤ ìš´ì˜ ë¶„ì„ (ì˜¨í†¨ë¡œì§€ ShipmentOperation í´ë˜ìŠ¤)"""
        if self.invoice_df is None:
            return {}
        
        df = self.invoice_df.copy()
        
        # 1. ì›”ë³„ ìš´ì˜ ë¶„ì„
        monthly_ops = df.groupby('operation_month').agg({
            'shipment_no': 'nunique',
            'packages_qty': 'sum',
            'weight_kg': 'sum',
            'cbm': 'sum',
            'total_cost': 'sum',
            'handling_in': 'sum',
            'handling_out': 'sum'
        }).fillna(0)
        
        # 2. ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        category_analysis = df.groupby('category').agg({
            'shipment_no': 'nunique',
            'packages_qty': 'sum',
            'weight_kg': 'sum',
            'cbm': 'sum',
            'total_cost': 'sum'
        }).fillna(0)
        
        # 3. HE íŒ¨í„´ ë¶„ì„
        he_pattern_analysis = df[df['extracted_he_pattern'].notna()].groupby('extracted_he_pattern').agg({
            'shipment_no': 'nunique',
            'packages_qty': 'sum',
            'total_cost': 'sum'
        }).fillna(0)
        
        # 4. ë¹„ìš© êµ¬ì¡° ë¶„ì„ (CostStructure í´ë˜ìŠ¤)
        cost_structure = {
            'total_handling_in': df['handling_in'].sum(),
            'total_handling_out': df['handling_out'].sum(),
            'total_cost': df['total_cost'].sum(),
            'avg_cost_per_shipment': df['total_cost'].sum() / df['shipment_no'].nunique() if df['shipment_no'].nunique() > 0 else 0,
            'avg_cost_per_package': df['total_cost'].sum() / df['packages_qty'].sum() if df['packages_qty'].sum() > 0 else 0
        }
        
        return {
            'monthly_operations': monthly_ops,
            'category_analysis': category_analysis,
            'he_pattern_analysis': he_pattern_analysis,
            'cost_structure': cost_structure,
            'summary': {
                'total_shipments': df['shipment_no'].nunique(),
                'total_packages': df['packages_qty'].sum(),
                'total_weight_kg': df['weight_kg'].sum(),
                'total_cbm': df['cbm'].sum(),
                'total_cost': df['total_cost'].sum(),
                'unique_he_patterns': df['extracted_he_pattern'].nunique()
            }
        }
    
    def create_invoice_ontology_data(self) -> Dict[str, pd.DataFrame]:
        """ì¸ë³´ì´ìŠ¤ ë°ì´í„°ë¥¼ ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ë³„ë¡œ êµ¬ì¡°í™”"""
        if self.invoice_df is None:
            return {}
        
        df = self.invoice_df.copy()
        ontology_data = {}
        
        # InvoiceRecord í´ë˜ìŠ¤
        invoice_records = df[['shipment_no', 'operation_month', 'category', 'extracted_he_pattern']].copy()
        invoice_records = invoice_records.dropna(subset=['shipment_no'])
        ontology_data['InvoiceRecord'] = self.mapper.map_dataframe_columns(invoice_records, 'InvoiceRecord')
        
        # ShipmentOperation í´ë˜ìŠ¤
        shipment_ops = df[['shipment_no', 'packages_qty', 'weight_kg', 'cbm', 'start_date', 'finish_date']].copy()
        shipment_ops = shipment_ops.dropna(subset=['shipment_no'])
        ontology_data['ShipmentOperation'] = self.mapper.map_dataframe_columns(shipment_ops, 'ShipmentOperation')
        
        # CostStructure í´ë˜ìŠ¤
        cost_structures = df[['shipment_no', 'total_cost', 'handling_in', 'handling_out']].copy()
        cost_structures = cost_structures.dropna(subset=['shipment_no'])
        ontology_data['CostStructure'] = self.mapper.map_dataframe_columns(cost_structures, 'CostStructure')
        
        print(f"ğŸ§  ì¸ë³´ì´ìŠ¤ ì˜¨í†¨ë¡œì§€ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(ontology_data)}ê°œ í´ë˜ìŠ¤")
        
        return ontology_data

# =============================================================================
# 3. SIMPLIFIED WAREHOUSE ANALYZER (ì˜¨í†¨ë¡œì§€ íŒŒì´í”„ë¼ì¸ ì—†ì´)
# =============================================================================

class SimpleWarehouseAnalyzer:
    """ê°„ë‹¨í•œ ì°½ê³  ë°ì´í„° ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.warehouse_data = {}
    
    def load_warehouse_data(self, data_dir: str = "data") -> bool:
        """ì°½ê³  ë°ì´í„° ë¡œë“œ"""
        try:
            warehouse_files = [
                'HVDC WAREHOUSE_HITACHI(HE).xlsx',
                'HVDC WAREHOUSE_HITACHI(HE-0214,0252)1.xlsx', 
                'HVDC WAREHOUSE_HITACHI(HE_LOCAL).xlsx',
                'HVDC WAREHOUSE_SIMENSE(SIM).xlsx'
            ]
            
            all_cases = []
            monthly_data = []
            
            for filename in warehouse_files:
                file_path = os.path.join(data_dir, filename)
                if os.path.exists(file_path):
                    try:
                        df = pd.read_excel(file_path, sheet_name=0)
                        
                        # Case No ì¶”ì¶œ
                        if 'Case No.' in df.columns:
                            cases = df['Case No.'].dropna().unique().tolist()
                            all_cases.extend(cases)
                            print(f"âœ… {filename}: {len(cases)}ê°œ ì¼€ì´ìŠ¤")
                        
                        # ë‚ ì§œ ì»¬ëŸ¼ì—ì„œ ì›”ë³„ ë°ì´í„° ì¶”ì¶œ
                        date_columns = [col for col in df.columns if self._is_date_column(df[col])]
                        
                        for _, row in df.iterrows():
                            case_no = row.get('Case No.', 'UNKNOWN')
                            qty = pd.to_numeric(row.get("Q'ty", 1), errors='coerce') or 1
                            
                            for date_col in date_columns:
                                if pd.notna(row[date_col]):
                                    date_val = pd.to_datetime(row[date_col], errors='coerce')
                                    if pd.notna(date_val):
                                        monthly_data.append({
                                            'Case_No': case_no,
                                            'Date': date_val,
                                            'YearMonth': date_val.strftime('%Y-%m'),
                                            'Location': str(date_col),
                                            'Qty': qty,
                                            'Source_File': filename
                                        })
                        
                    except Exception as e:
                        print(f"âš ï¸ {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            self.warehouse_data = {
                'cases': all_cases,
                'monthly_data': pd.DataFrame(monthly_data),
                'total_cases': len(all_cases)
            }
            
            print(f"âœ… ì°½ê³  ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(all_cases)}ê°œ ì¼€ì´ìŠ¤, {len(monthly_data)}ê°œ ì´ë²¤íŠ¸")
            return True
            
        except Exception as e:
            print(f"âŒ ì°½ê³  ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _is_date_column(self, series: pd.Series) -> bool:
        """ì»¬ëŸ¼ì´ ë‚ ì§œ ë°ì´í„°ì¸ì§€ í™•ì¸"""
        sample_values = series.dropna().head(10)
        if len(sample_values) == 0:
            return False
        
        date_count = 0
        for val in sample_values:
            try:
                pd.to_datetime(val)
                date_count += 1
            except:
                pass
        
        return date_count / len(sample_values) > 0.5
    
    def analyze_warehouse_operations(self) -> Dict[str, Any]:
        """ì°½ê³  ìš´ì˜ ë¶„ì„"""
        if not self.warehouse_data or self.warehouse_data['monthly_data'].empty:
            return {}
        
        monthly_df = self.warehouse_data['monthly_data']
        
        # ì›”ë³„ ì§‘ê³„
        monthly_summary = monthly_df.groupby('YearMonth').agg({
            'Case_No': 'nunique',
            'Qty': 'sum'
        }).reset_index()
        monthly_summary.columns = ['YearMonth', 'Cases', 'Quantity']
        
        # ìœ„ì¹˜ë³„ ì§‘ê³„
        location_summary = monthly_df.groupby('Location').agg({
            'Case_No': 'nunique',
            'Qty': 'sum'
        }).reset_index()
        location_summary.columns = ['Location', 'Cases', 'Quantity']
        
        return {
            'monthly_summary': monthly_summary,
            'location_summary': location_summary,
            'summary': {
                'total_cases': self.warehouse_data['total_cases'],
                'total_events': len(monthly_df),
                'unique_months': monthly_df['YearMonth'].nunique(),
                'unique_locations': monthly_df['Location'].nunique()
            }
        }

# =============================================================================
# 4. INTEGRATED ANALYZER
# =============================================================================

class IntegratedAnalyzer:
    """í†µí•© ë¶„ì„ê¸° (ì°½ê³  + ì¸ë³´ì´ìŠ¤)"""
    
    def __init__(self):
        self.mapper = EnhancedOntologyMapper("mapping_rules_v2.4.json")
        self.invoice_analyzer = InvoiceAnalyzer(self.mapper)
        self.warehouse_analyzer = SimpleWarehouseAnalyzer()
        self.integrated_results = {}
    
    def load_all_data(self) -> bool:
        """ëª¨ë“  ë°ì´í„° ë¡œë“œ"""
        try:
            # ì¸ë³´ì´ìŠ¤ ë°ì´í„° ë¡œë“œ
            if not self.invoice_analyzer.load_invoice_data():
                return False
            
            # ì°½ê³  ë°ì´í„° ë¡œë“œ
            if not self.warehouse_analyzer.load_warehouse_data():
                return False
            
            print(f"âœ… í†µí•© ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def perform_integrated_analysis(self) -> Dict[str, Any]:
        """í†µí•© ë¶„ì„ ìˆ˜í–‰"""
        print("ğŸ”„ í†µí•© ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        
        # 1. ì¸ë³´ì´ìŠ¤ ë¶„ì„
        invoice_analysis = self.invoice_analyzer.analyze_invoice_operations()
        
        # 2. ì°½ê³  ë¶„ì„
        warehouse_analysis = self.warehouse_analyzer.analyze_warehouse_operations()
        
        # 3. í†µí•© ì—°ê´€ ë¶„ì„
        integration_analysis = self._analyze_integration()
        
        self.integrated_results = {
            'invoice_analysis': invoice_analysis,
            'warehouse_analysis': warehouse_analysis,
            'integration_analysis': integration_analysis,
            'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return self.integrated_results
    
    def _analyze_integration(self) -> Dict[str, Any]:
        """í†µí•© ì—°ê´€ ë¶„ì„"""
        try:
            # ì‹œê°„ì  ì¤‘ë³µ ë¶„ì„
            invoice_months = set()
            if 'monthly_operations' in self.invoice_analyzer.analyze_invoice_operations():
                monthly_ops = self.invoice_analyzer.analyze_invoice_operations()['monthly_operations']
                invoice_months = set(monthly_ops.index.strftime('%Y-%m'))
            
            warehouse_months = set()
            warehouse_analysis = self.warehouse_analyzer.analyze_warehouse_operations()
            if 'monthly_summary' in warehouse_analysis:
                warehouse_months = set(warehouse_analysis['monthly_summary']['YearMonth'])
            
            common_months = invoice_months.intersection(warehouse_months)
            
            # HE íŒ¨í„´ ë§¤ì¹­ ë¶„ì„
            invoice_he_patterns = set()
            if self.invoice_analyzer.invoice_df is not None:
                invoice_he_patterns = set(self.invoice_analyzer.invoice_df['extracted_he_pattern'].dropna().unique())
            
            warehouse_cases = self.warehouse_analyzer.warehouse_data.get('cases', [])
            
            # íŒ¨í„´ ë§¤ì¹­ ì‹œë„
            potential_matches = 0
            for he_pattern in invoice_he_patterns:
                he_number = he_pattern.replace('HE-', '').lstrip('0')
                if he_number:
                    matching_cases = [case for case in warehouse_cases if he_number in str(case)]
                    if matching_cases:
                        potential_matches += 1
            
            return {
                'temporal_analysis': {
                    'invoice_months': len(invoice_months),
                    'warehouse_months': len(warehouse_months),
                    'common_months': len(common_months),
                    'temporal_overlap_rate': len(common_months) / max(len(invoice_months), len(warehouse_months)) * 100 if max(len(invoice_months), len(warehouse_months)) > 0 else 0
                },
                'pattern_analysis': {
                    'invoice_he_patterns': len(invoice_he_patterns),
                    'warehouse_cases': len(warehouse_cases),
                    'potential_matches': potential_matches,
                    'pattern_match_rate': potential_matches / len(invoice_he_patterns) * 100 if invoice_he_patterns else 0
                },
                'data_quality': {
                    'invoice_completeness': len(self.invoice_analyzer.invoice_df) if self.invoice_analyzer.invoice_df is not None else 0,
                    'warehouse_completeness': len(warehouse_cases)
                }
            }
            
        except Exception as e:
            print(f"âŒ í†µí•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

# =============================================================================
# 5. REPORT GENERATOR
# =============================================================================

def create_comprehensive_report(analyzer: IntegratedAnalyzer, output_filename: str = None):
    """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
    if not analyzer.integrated_results:
        print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if output_filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_filename = f'HVDC_í†µí•©_ì˜¨í†¨ë¡œì§€_ì¸ë³´ì´ìŠ¤_ë¦¬í¬íŠ¸_{timestamp}.xlsx'
    
    try:
        with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:
            results = analyzer.integrated_results
            
            # 1. ì¢…í•©ìš”ì•½ ì‹œíŠ¸
            summary_data = []
            
            # ì¸ë³´ì´ìŠ¤ ìš”ì•½
            invoice_summary = results['invoice_analysis']['summary']
            summary_data.extend([
                ['ğŸ“‹ ì¸ë³´ì´ìŠ¤ ë¶„ì„ ìš”ì•½', ''],
                ['ì´ ì„ ì  ê±´ìˆ˜', f"{invoice_summary['total_shipments']:,}ê±´"],
                ['ì´ íŒ¨í‚¤ì§€ ìˆ˜ëŸ‰', f"{invoice_summary['total_packages']:,}ê°œ"],
                ['ì´ ì¤‘ëŸ‰', f"{invoice_summary['total_weight_kg']:,.1f}kg"],
                ['ì´ ë¶€í”¼', f"{invoice_summary['total_cbm']:,.2f}CBM"],
                ['ì´ ë¹„ìš©', f"${invoice_summary['total_cost']:,.2f}"],
                ['ê³ ìœ  HE íŒ¨í„´', f"{invoice_summary['unique_he_patterns']:,}ê°œ"],
                ['', ''],
            ])
            
            # ì°½ê³  ìš”ì•½
            warehouse_summary = results['warehouse_analysis']['summary']
            summary_data.extend([
                ['ğŸ¢ ì°½ê³  ë¶„ì„ ìš”ì•½', ''],
                ['ì´ ì¼€ì´ìŠ¤', f"{warehouse_summary['total_cases']:,}ê°œ"],
                ['ì´ ì´ë²¤íŠ¸', f"{warehouse_summary['total_events']:,}ê±´"],
                ['ìš´ì˜ ì›”ìˆ˜', f"{warehouse_summary['unique_months']:,}ê°œì›”"],
                ['ìš´ì˜ ìœ„ì¹˜', f"{warehouse_summary['unique_locations']:,}ê°œì†Œ"],
                ['', ''],
            ])
            
            # í†µí•© ë¶„ì„
            integration = results['integration_analysis']
            temporal = integration.get('temporal_analysis', {})
            pattern = integration.get('pattern_analysis', {})
            
            summary_data.extend([
                ['ğŸ”— í†µí•© ë¶„ì„ ê²°ê³¼', ''],
                ['ì‹œê°„ì  ì¤‘ë³µë¥ ', f"{temporal.get('temporal_overlap_rate', 0):.1f}%"],
                ['íŒ¨í„´ ë§¤ì¹­ë¥ ', f"{pattern.get('pattern_match_rate', 0):.1f}%"],
                ['ê³µí†µ ìš´ì˜ ì›”ìˆ˜', f"{temporal.get('common_months', 0):,}ê°œì›”"],
                ['ì ì¬ì  ë§¤ì¹­', f"{pattern.get('potential_matches', 0):,}ê±´"],
            ])
            
            summary_df = pd.DataFrame(summary_data, columns=['í•­ëª©', 'ê°’'])
            summary_df.to_excel(writer, sheet_name='ğŸ“Š ì¢…í•©ìš”ì•½', index=False)
            
            # 2. ì¸ë³´ì´ìŠ¤ ì›”ë³„ ë¶„ì„
            if 'monthly_operations' in results['invoice_analysis']:
                monthly_df = results['invoice_analysis']['monthly_operations'].reset_index()
                monthly_df.columns = ['ìš´ì˜ì›”', 'ì„ ì ê±´ìˆ˜', 'íŒ¨í‚¤ì§€ìˆ˜ëŸ‰', 'ì¤‘ëŸ‰(kg)', 'CBM', 'ì´ë¹„ìš©', 'ì…ê³ ì²˜ë¦¬ë¹„', 'ì¶œê³ ì²˜ë¦¬ë¹„']
                monthly_df.to_excel(writer, sheet_name='ğŸ’° ì¸ë³´ì´ìŠ¤_ì›”ë³„_ë¶„ì„', index=False)
            
            # 3. ì¸ë³´ì´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
            if 'category_analysis' in results['invoice_analysis']:
                category_df = results['invoice_analysis']['category_analysis'].reset_index()
                category_df.columns = ['ì¹´í…Œê³ ë¦¬', 'ì„ ì ê±´ìˆ˜', 'íŒ¨í‚¤ì§€ìˆ˜ëŸ‰', 'ì¤‘ëŸ‰(kg)', 'CBM', 'ì´ë¹„ìš©']
                category_df.to_excel(writer, sheet_name='ğŸ“¦ ì¸ë³´ì´ìŠ¤_ì¹´í…Œê³ ë¦¬ë³„', index=False)
            
            # 4. ì°½ê³  ì›”ë³„ ë¶„ì„
            if 'monthly_summary' in results['warehouse_analysis']:
                warehouse_monthly_df = results['warehouse_analysis']['monthly_summary']
                warehouse_monthly_df.to_excel(writer, sheet_name='ğŸ¢ ì°½ê³ _ì›”ë³„_ë¶„ì„', index=False)
            
            # 5. HE íŒ¨í„´ ë¶„ì„
            if 'he_pattern_analysis' in results['invoice_analysis']:
                he_pattern_df = results['invoice_analysis']['he_pattern_analysis'].reset_index()
                he_pattern_df.columns = ['HEíŒ¨í„´', 'ì„ ì ê±´ìˆ˜', 'íŒ¨í‚¤ì§€ìˆ˜ëŸ‰', 'ì´ë¹„ìš©']
                he_pattern_df.to_excel(writer, sheet_name='ğŸ”— HEíŒ¨í„´_ë¶„ì„', index=False)
        
        print(f"âœ… í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_filename}")
        return output_filename
        
    except Exception as e:
        print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# =============================================================================
# 6. MAIN EXECUTION
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ HVDC í†µí•© ì˜¨í†¨ë¡œì§€ ì¸ë³´ì´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 60)
    
    # í†µí•© ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = IntegratedAnalyzer()
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    if not analyzer.load_all_data():
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # í†µí•© ë¶„ì„ ìˆ˜í–‰
    print("\nğŸ”„ í†µí•© ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
    results = analyzer.perform_integrated_analysis()
    
    if not results:
        print("âŒ ë¶„ì„ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
    print("-" * 40)
    
    # ì¸ë³´ì´ìŠ¤ ìš”ì•½
    invoice_summary = results['invoice_analysis']['summary']
    print(f"ğŸ“‹ ì¸ë³´ì´ìŠ¤ ë¶„ì„:")
    print(f"   - ì´ ì„ ì  ê±´ìˆ˜: {invoice_summary['total_shipments']:,}ê±´")
    print(f"   - ì´ íŒ¨í‚¤ì§€: {invoice_summary['total_packages']:,}ê°œ")
    print(f"   - ì´ ë¹„ìš©: ${invoice_summary['total_cost']:,.2f}")
    print(f"   - HE íŒ¨í„´: {invoice_summary['unique_he_patterns']:,}ê°œ")
    
    # ì°½ê³  ìš”ì•½
    warehouse_summary = results['warehouse_analysis']['summary']
    print(f"\nğŸ¢ ì°½ê³  ë¶„ì„:")
    print(f"   - ì´ ì¼€ì´ìŠ¤: {warehouse_summary['total_cases']:,}ê°œ")
    print(f"   - ì´ ì´ë²¤íŠ¸: {warehouse_summary['total_events']:,}ê±´")
    print(f"   - ìš´ì˜ ì›”ìˆ˜: {warehouse_summary['unique_months']:,}ê°œì›”")
    
    # í†µí•© ë¶„ì„
    integration = results['integration_analysis']
    temporal = integration.get('temporal_analysis', {})
    pattern = integration.get('pattern_analysis', {})
    
    print(f"\nğŸ”— í†µí•© ë¶„ì„:")
    print(f"   - ì‹œê°„ì  ì¤‘ë³µë¥ : {temporal.get('temporal_overlap_rate', 0):.1f}%")
    print(f"   - íŒ¨í„´ ë§¤ì¹­ë¥ : {pattern.get('pattern_match_rate', 0):.1f}%")
    
    # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    print(f"\nğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    output_file = create_comprehensive_report(analyzer)
    
    if output_file:
        print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼: {output_file}")
        
        # ì˜¨í†¨ë¡œì§€ ë°ì´í„° ì¶œë ¥
        print(f"\nğŸ§  ì˜¨í†¨ë¡œì§€ ë°ì´í„° êµ¬ì¡°í™”...")
        invoice_ontology = analyzer.invoice_analyzer.create_invoice_ontology_data()
        if invoice_ontology:
            ttl_file = output_file.replace('.xlsx', '_ontology.ttl')
            analyzer.mapper.export_to_ttl(invoice_ontology, ttl_file)
    else:
        print(f"\nâŒ ë¦¬í¬íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 